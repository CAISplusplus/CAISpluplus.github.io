---
layout: post
published: true
title: "Lesson 4: Neural Networks, Part 1 (Architecture)"
headline: "The heart of Deep Learning"
mathjax: true
featured: true
categories: curriculum 

comments: false
---

<!-- Introduction -->
## Introduction

When one writes a computer program they break their problem into multiple
steps and define procedural steps in their code for the computer to
execute. The beauty of machine learning is the programmer does not tell the
computer how to solve the problem. Instead, the computer learns how to solve
the problem from **data**. 


If you have heard of machine learning before, chances are you have heard of
neural networks as well. Neural networks are often called **artificial
neural networks** because their structure is loosely modeled off of the
neuron structure in the brain. The human brain contains around 100 billion
neurons. Each neuron is a small cell that transmits signals, and is connected
to other neurons via their synapses. These small
neuron units and their connections when linked together give humans the
capability to think. 


A similar approach was taken to the machine learning models that are called
artificial neural networks. In an artificial neural network, many units (neurons)
function together to give the system the capability
to learn from data. Like the brain, these neurons are connected to each other. The
exact structure of neural networks and how these connections and neurons work
will be discussed in the following pages. 


So why are neural networks so popular?  In recent years, neural networks have
exploded in popularity.  Cheaper and more powerful hardware has allowed researchers
to link together thousands, and even millions, of artificial neurons into artificial neural networks,
allowing for a more powerful extension of machine learning called **deep learning**. 
This renaissance in deep learning has seen countless breakthroughs in neural networks
and their applications. The second reason is the versatility of neural
networks. With their highly flexible architecture, neural networks have found applications 
in text, speech, images and much more. They have proven to be a useful machine learning 
tool in a variety of situations. 

For better or for worse, there is a good amount of math associated with neural networks. This post
will go into much of the surface level theory that goes into a neural
network. From this theory, you should gain a good understanding of how basic
neural networks work, which will then allow you to work on actual implementations.

**Note**: Since the material in this lesson may be more challenging than in previous lesson, we have included both video links and written content. Feel free to choose whichever medium
is best for your style of learning, as long as you make sure that you understand the material. If you choose to focus on the videos, it would probably be a good idea to at least skim through the written content and do some quick checks for understanding.


## Intro to Deep Learning

Before we delve too deep into the theory behind neural networks, it may help to see how neural networks, which are the key enabler of deep learning, relate to linear and logistic regression (which are examples of traditional machine learning). This 11-minute video should give you a good idea of why neural networks are a more flexible tool than simple linear regression, and what the connections between neurons in neural networks are actually doing. Lucky for us, the speaker even explains these ideas in terms of the housing price prediction problem that we discussed earlier.

<p style="text-align:center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/l42lr8AlrHk" frameborder="0" allowfullscreen></iframe>
</p>

**Recap**: In order to make truly accurate housing price predictions, we may need more information than just the inputs given to us in their current form (e.g. number of bedrooms, bathrooms, etc). In many instances, *combinations* of these features (e.g. 2 bedrooms **and** 2 bathrooms) can be just as important as the original features to the final price. While pure linear regression won't let us take this into account, neural networks will. Neural networks work by combining *layers* of neurons, which allows the network to learn these relevant combinations of features and use them to generate more accurate predictions. After the input layer, each successive layer of neurons learns to detect some increasingly abstract combinations of features from the previous layer, until we finally use these learned compound-features to make a final prediction.

This layering of features may not seem too groundbreaking right now, but you'll eventually see how this concept of learning more and more abstract combinations of features will allow us to build up representations of much more complex types of data, such as images, text, etc.


## Neural Network Architecture: Video Content

In order to see how the actual math behind neural networks work, let's look at another example problem: this time, let's try to predict test scores based on two input features: hours studied and hours slept. Along the way, you'll see how the layers of neurons in neural networks can use a series of mathematical operations to transform some input data and use it to make a final prediction.

### Video 1: Data and Architecture

<p style="text-align:center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/bxe2T-V8XRs" frameborder="0" allowfullscreen></iframe>
</p>

**Recap:** At a completely abstract level, what a neural network is doing is taking in some inputs $$X$$ (e.g. hours studied and hours slept), applying some transormations, and then generating an outputted prediction $$y$$ (e.g. test score). Since our inputs can be in different units, we must first scale/normalize all our input data so that our model can "compare apples to apples". Between our input and output layers, we will have some hidden layers made up of artificial neurons. Each neuron takes in the output from the previous layer, scales each piece of input data by some weights (much like in linear/logistic regression), sums up over these weighted inputs, and then applies an activation function (see notes below). This final activation then gets passed to the neurons of the next layer as a new series of inputs. 

**Notes**:
* The video leaves this out, but in addition to summing up over the weighted inputs, each neuron also adds on a "bias" term before applying the activation function. If you're confused on what each weight/bias in a neuron actually represents (aside from the mathematical operations), we highly recommend that you take a look at our written explanation below: [**Neuron Intuition**](#neuron-intuition).
* The activation function that the speaker mentions at the 2:50 mark is essentially a type of "squashing function". We use activation functions to make sure that the scale of our output makes sense. For example, we can use the sigmoid activation function to squash our output between 0 and 1 (which is great for outputting probabilities). After appyling the sigmoid activation function, more negative values will end up closer to 0, and more positive values will end up closer to 1. See our section on this below for more info: [**Activation Functions**](#activation-functions).
* In these simple neural networks, every neuron in any given hidden layer takes in all the outputs from the previous layer as inputs. For these reasons, these layers are called "fully connected" layers. Remember this terminology, since eventually, we'll begin to discuss neural network layers that don't fit these criteria.


### Video 2: Forward Propagation

<p style="text-align:center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/UJwK6jAStmg" frameborder="0" allowfullscreen></iframe>
</p>

**Recap:** Before training the model, we must decide on some hyperparameters for our model: these include values like the number of hidden layers in our network, how many neurons will be in each hidden layer, etc. The actual automated learning process takes place in the weights of the network, which are similar to the model weights that we saw in linear/logistic regression. 

We can perform the data transformations described above by placing these weights in a matrix, and then multiplying the input matrix by this weight matrix. The result is then fed through our activation function to squash the values to within our desired range, finally giving us the outputs of the hidden layer. To get from our hidden layer to our final prediction, we once again multiply the outputs of the hidden layer by our last weight matrix and apply our activation, giving us a final prediction that lies within our desired range.

**Notes:**
* This process is called "Forward Propagation" because we start with our input data and propagate it forward through the layers of our network, applying matrix multiplications and activation functions until we end up with our final result.
* Since we started with random weights and haven't yet trained our network, this neural network is still pretty much useless when it comes to generating meaningful outputs. In the next lesson, we'll see how we can train a neural network on inputted data so that it can start to make accurate predictions. 

(Sneak peek: once again, we'll be measuring the error of our network using a cost function, and applying our good friend gradient descent on the weights of our neural network to minimize this cost function. The goal is that after optimizing our model weights, propagating our inputs forward through the network will cause the input data to become transformed in such a way that the network outputs a reasonable final result. However, now that we have more than one layer, it becomes a lot tricker to track how changing one weight affects the overall cost, so we'll have to do some extra book-keeping before we can actually apply gradient descent. Stay tuned to see how we can solve this conundrum!)

<br>

## Neuron Network Architecture: Written Content

What is a neural network? Let's begin by dissecting the name itself.
In mathematics, a network can be though of as a graph and is nothing more than a set of
vertices or *nodes* that are connected via arrows, or *edges*. They can be used to represent
everything from connections within friend groups (mathematical "social networks") to 
connections between artificial neurons, as we'll see below.
An example of a network is shown here. 

<img class='center-image' src='/assets/img/ml/crash_course/directed_acyclic_graph.png' />

The other important aspect of our neural network is, of course, the "neuron." We
want our network to be able to understand and learn. While this might seem
far fetched we will see that neural networks use these graphs as a powerful
structure to define operations to learn desired responses. 
Similar to the historical beginnings of
neural networks, let's use inspirations of biology to get started. 

### The Neuron

Examine an individual node in our graph. Inspired by the brain we 
will call this a neuron. An illustration of an artificial neuron is shown below. 

<img class='center-image' src='/assets/img/ml/crash_course/single-neuron.png' width="40%"/>

Before we get into the actual intuition of what a neuron is, let's get
through the math behind one, since the artificial neuron is at its core a mathematical construct.

<p>
    The input of the incoming edge is notated as the scalar \( p \). (The videos above used \( x \) instead to represent the inputs -- same idea, just different notation.) This edge has 
    a weight of the scalar \( w \). The weight is multiplied by the input to form the value 
    \( wp \). This is then sent into the the summation block which sums the 
    input \( wp \) and the bias \( b \). Notice that the bias has no 
    dependence on external input. Summing these two terms together then 
    gives \( wp + b \) or in the above image \( n \).
</p>

<p>
    The output of the summation \( n \) is then passed through the activation 
    function \( f \). The activation function is just some real valued scalar
    function. This then gives the final output of the neuron 
    \( f(n) = a \). The neuron output can then be calculated as 
    $$ a = f(wp + b) $$
</p>

### <a name="neuron_intuition"></a>Neuron Intuition

<p>
  So what is the intuition behind a neuron? We can view the output of this
  neuron as making a <i>decision</i>.  This decision is based on the
  inputs, weights and bias of the neuron. Let's say you are trying to make the
  decision of if you want to go to a party tonight. Let's say we live in a
  world where this decision depends on only two factors: are you tired and is
  your best friend at the party? Note that these factors are simple yes or no
  questions. We can <i>encode</i> yes as \( 1 \) and no as \( 0 \). 
</p>

<p>
  The importance of these two factors will vary a lot from person to person.
  This corresponds to different weight values. For this neuron say that our
  activation function is the simple linear function \( f(x) = x \) where we say
  that any output \( > 0 \) means we should decide to go to the party and any
  output \( < 0 \) means that we should decide to not go. A normal person would
  not want to go to a party while tired. We should then make the weight (\(w_0
  \)) for the are you tired input (\( x_0 \)) be negative. On the other hand, you would 
  hopefully want to go if your best friend going, so the  weight (\(w_1\)) for that input
  (\(x_1\)) would be positive. 
</p>

<p>
  Say that you absolutely hate going out when you are tired and this is far
  more important than if your best friend is at the party. We could make \(
  w_0 = -10 \) and \( w_1 = 1 \) to represent this. If you  are tired you will
  never go out even if your best friend is there, because \( -10 + 1 < 0 \). However, if
  your best friend is there but you are not tired you would still go out, because \( 0 +
  1 > 0 \). If you were not tired and your best friend wasn't there, you would
  be right on the decision boundary and could just choose randomly.
</p>

<p>
  Now that we have an idea of a decision boundary set up,
  we can now incorporate <i>bias</i> to change our decision boundary. When we had no
  bias in the previous example, the decision boundary (or cutoff) was 0. If we set the bias
  to be \( 1 \), the decision boundary will be \( 1 \). A more positive decision
  boundary means we are less inclined to go to parties given any inputs, 
  since we would need a higher score to be above the cutoff point. Let's
  change the problem slightly for \( x_1 \) to be the number of your friends
  that are going. If you generally enjoy going to parties, your decision neuron
  could have \( b = -2 \), and of course you want your friends to be there, and
  do not like going to a party while tired, so \( w_0 = -4, w_1 = 1 \). So even
  if you are tired, it would only take three of your friends to be there for you
  to want to go to the party. But if \( b = 0 \), it would take five friends if
  you are tired.
</p>

### <a name="activation-functions"></a>Activation Functions

<p>
  In our example, we chose the linear activation function, where our equation took
  the form \( a = w_0 x_0 + w_1 x_1 + b \). 
  Our simple linear linear activation function would look like the
  below for input \( p \) and output \( a \). 
</p>

<img class='center-image' src='/assets/img/ml/crash_course/pure-linear-transform.png' />

<p>
  However, we should probably tell you that
  there are a variety of other activation functions that are employed
  in neurons. 

  Going back to the decision about the party, say there is another person that
  is trying to <i>predict</i> if you are going to go to the party. In this case, we would want our output to be a probability (between 0-1), so our earlier cut-off rule will not apply. We could just take the
  pure score value, and based on how positive or negative it is, determine how certain
  you are to go to the party. However, there is a function called the
  <i>sigmoid</i> function that does a better job of representing these
  probabilistic outputs. Any probability can be represented between 0 and 1.
  The sigmoid function does just this, by squashing any real value to fit between 0 and 1. 
  Below is an image of the sigmoid function in action. 
</p>

<img class='center-image' src='/assets/img/ml/crash_course/sigmoid.png' />

<p>
  After applying the sigmoid function, very negative outputs (which, using our previous cutoff rule, would make us not want to go to the party) will produce values close to 0; very positive
  outputs (which would make us want to go to the party) will produce outputs close to 1. 
</p>

<p>
  Neural networks are probabilistic systems, and therefore functions like the
  sigmoid function are a lot more powerful than just the linear activation
  function. We will see why the sigmoid function and other non-linear functions
  are so powerful in later lessons.
</p>

### Vector Formulation

<p>
  Before moving on let's clean up some of the math behind what we have
  developed with the neuron so far. Say we have the multiple input neuron
  pictured below. 
</p>

<img class='center-image' src='/assets/img/ml/crash_course/multiple_input_neuron.png' width="40%"/>

<p>
  Each of the inputs to the node can just be represented as a vector to make
  the representation easier. 
  $$ \textbf{p} = \begin{bmatrix}
                    p_1 \\
                    p_2 \\
                    p_3 \\
                    \vdots \\
                    p_R
                  \end{bmatrix}$$
  Know that bold face represents a vector. 
</p>
<p>
  Likewise, we can also formulate the list of weights for each input value as a
  vector. 
  $$
  \textbf{w}_1 =  \begin{bmatrix}
                    w_{1,1},
                    w_{1,2},
                    w_{1,3} ,
                    \dots 
                    w_{1, R}
                  \end{bmatrix}
  $$
  Note that the weight vector is a row vector (not a column vector); 
  due to the way matrix/vector multiplication works,
  this will become necessary for when we will have to multiply this weight vector with the input vector. 
</p>

<p>
  Just as before, we are simply multiplying the inputs by their corresponding weights. So our new
  vector would be just to multiply each input by the weight on that edge. 
  $$
  \begin{bmatrix}
    w_{1,1} p_1,
    w_{1,2} p_2,
    w_{1,3} p_3,
    \dots
    w_{1,R} p_R
  \end{bmatrix}
  $$
  This is the same as \( \textbf{w}_1 \textbf{p}\). 
</p>

<p>
  The next step is to go through the summation. Summing up the components of this
  vector gives 
  $$ w_{1,1} p_1 + w_{1,2} p_2 + w_{1,3} p_3 + \dots + w_{1, R} p_R $$
  Then we add in the bias \(b\), which, as before, is just a single scalar.
  $$ n = w_{1,1} p_1 + w_{1,2} p_2 + w_{1,3} p_3 + \dots + w_{1, R} p_R + b =
  \textbf{w}_1 \textbf{p} + b$$
</p>

<p>
  Note that this entire expression \(n=\textbf{w}_1 \textbf{p}+b\) is still just a scalar. We then
  transform the input by the activation function to get the final output of the
  node. 
  $$ a = f(\textbf{w}_1 \textbf{p} + b )$$
</p>

This one equation pretty much sums up everything that a single artificial neuron does: multiplying the neuron's inputs by their corresponding weights, summing up these weighted inputs (along with a bias term), and then applying an activation function at the end to squash the output to within a desired range. 

The next step is to see what happens once we start working with multiple neurons, which we can combine vertically to form layers. We will first look at the case where we have just one layer of neurons.

### Layers of Neurons

<p>
  We know that the weights of a neuron control how a decision is made by the
  neuron. Different weights will give a neuron different decision properties.
  What if we wanted to work with multiple neurons each having different weights
  at the same time? We could do this by stacking the neurons into a
  <i>layer</i> of neurons, where the inputs are fed into each neuron in
  parallel. 
</p>

<img class='center-image' src='/assets/img/ml/crash_course/neuron_layer.png' width="30%"/>

Now, the same principle applies as before: only now that we have multiple neurons, each neuron will have its own weight vector $$w_i$$. Each neuron will use its own weights to generate a different output $$a_i$$ .

<p>
  For node \(i\), calculate the output
  \(a_i\) through the following formula. Note that we are assuming that all of
  the activation functions are the same, which is a safe
  assumption to make for this case.
  $$ a_i = f(\textbf{w}_i \textbf{p} + b) $$
  However, we can simplify this and view the weights as a matrix of weights
  represented as follows saying there are \(S\) nodes that the input is being fed
  into.
  $$ \textbf{W} = 
  \begin{bmatrix}
    w_{1,1} & w_{1,2} & w_{1,3} & \dots & w_{1,R} \\
    w_{2,1} & w_{2,2} & w_{2,3} & \dots & w_{2,R} \\
    w_{3,1} & w_{3,2} & w_{3,3} & \dots & w_{3,R} \\
    \dots & \dots & \dots & \dots & \dots \\
    w_{S,1} & w_{S,2} & w_{S,3} & \dots & w_{S,R} \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    \textbf{w}_1 \\
    \textbf{w}_2 \\
    \textbf{w}_3 \\
    \vdots \\
    \textbf{w}_S \\
  \end{bmatrix}
  $$
  We also no longer have a single bias but now a bias for each neuron and there
  are \(S\) neurons
  $$
  \textbf{b} = \begin{bmatrix}
  b_1,
  b_2,
  b_3,
  \dots
  b_S
  \end{bmatrix}
  $$

  Likewise, we can say there is an output vector \(\textbf{a}\) that can be
  calculated through the following formula.
  $$ \textbf{a} = f(\textbf{W} \textbf{p} + \textbf{b})$$
</p>

### Multiple Layers

<p>
  What happens if we feed the outputs of one layer of neurons into another
  layer of neurons? 
</p>

<img class='center-image' src='/assets/img/ml/crash_course/multiple_layers.png' width="60%"/>

<p>
  This is where we begin to see the power of neural networks. Each layer of
  neruon works on the abstraction of the previous layer. This allows deeper
  layers to make more complex and higher level decisions. Let's take a concrete
  example. Say you were builidng a neural network that takes as input
  handwritten images. The first layer could detect edges. The second could
  identify the contour the edges form. The third could take these contours and
  identify them with shapes. The fourth and final could take these shapes and
  associate them with numbers. 
</p>

<p>
  Now notate the weight matrix for layer \(i\) to be \(\textbf{W}^{i}\), and the 
  corresponding bias vector for layer layer \(i\) to be \(\textbf{b}^{i}\). Note
  that the number of neurons stacked vertically does not have to be the same in
  each layer. The neurons simply connect to all of the next level so it does
  not matter how neurons are in the previous layer for the current layer. The
  example below is a little more concrete of such a network.
</p>

<img class='center-image' src='/assets/img/ml/crash_course/network_example.png' width="60%"/>

<p>
  We can simply extend the rules used to compute the output of one layer and
  extend it to multiple layers. For instance to compute the output of the above
  network would be the following. 
  $$
  \textbf{a}^3 = f^3 ( \textbf{W}^3 f^2 ( \textbf{W}^2 f^1 (\textbf{W}^1
  \textbf{p} + \textbf{b}^1) + \textbf{b}^2) + \textbf{b}^3 )
  $$
  Notice how the input \(p\) is propagated from the left of the network to the
  right of the network.
</p>

<p>
  Typically you will see neural networks illustrated in the less expressive
  version shown below to save space. In the illustration each neuron is simply
  a node.
</p>

<img class='center-image' src='/assets/img/ml/crash_course/nn_illustration.png' />

### Training a One-Layer Network

To see how we can train simple one-layer neural networks to make more accurate predictions, see our supplementary write-up here: [**Lesson 4 Supplemental Material**](/curriculum/lesson4supplement). (Going through this isn't required, but if you're up for a challenge or just curious for a more mathematical preview of the Neural Network Training lesson, go ahead and give it a look.)

### Conclusion

<p>
  That is all for the basics of a neural network building blocks. You should be
  able to see how a neural network produces an output for some input. But what
  are all these transformations doing? The key is choosing the right values for
  the weights and the biases to make the network do interesting things. We can
  do this through having the network <b>learn</b> the weights and biases.
</p>

<p>
  We simply tell the neural network to learn, for some datset \(X,
  Y\), the mapping from \( X \) to \( Y \) and the network will find the
  appropriate weights to do so. 
</p>

<p>
  When it comes down to it, the neural network is still a statistical learner: 
  given a set of input data \(X\)
  and a set of output data \( Y \) a neural network can learn the mapping from
  \( X \) to \( Y \). 
</p>






<h2>Sources</h2>
<ul>
  <li>
    <a href='http://neuralnetworksanddeeplearning.com'>neuralnetworksanddeeplearning.com</a>
  </li>
  <li>
    <a href='http://hagan.okstate.edu/NNDesign.pdf'>
      Neural Network Design by Hagan
    </a>
  </li>
  <li>
    <a href='https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618'>
      Deep Learning (Adaptive Computation and Machine Learning Series) by
      Goodfellow, Bengio, Courville
    </a> 
  </li>
  <li>
    <a href='http://cs231n.github.io/'>CS231n Convolutional Neural Networks for
    Visual Recognition Class Notes</a>
  </li>
  <li>
    <a href='https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/'> 
      Why You Should Use Cross-Entropy Error Instead Of Classification Error Or Mean Squared Error For Neural Network Classifier Training by James D.  McCaffery
    </a>
  </li>
</ul>
