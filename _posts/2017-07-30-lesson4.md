---
layout: post
published: true
title: "Lesson 4: Neural Networks, Part 1"
headline: ""
mathjax: true
featured: true
categories: lessons

comments: false
---

<!-- Introduction -->
<h2>Introduction</h2>
<p>
  When one writes a computer program they break their problem into multiple
  steps and define procedural steps in their code for the computer to
  execute. The beauty of machine learning is the programmer does not tell the
  computer how to solve the problem. Instead, the computer learns how to solve
  the problem from <b>data</b>. 
</p>

<p>
  If you have heard of machine learning before, chances are you have heard of
  neural networks as well. Neural networks are often called <b>artificial
  neural networks</b> because their structure is loosely modeled off of the
  neuron structure in the brain. The human brain contains around 100 billion
  neurons. Each neuron is a small cell that transmits signals. The neurons are
  connected to each other through dendrites. These small
  neuron units and their connections give humans the
  capability to think. 
</p>

<p>
  A similar approach was taken to the learning machines that are called
  artificial neural networks. In an artificial neural network many units that
  are also called neurons function together to give the system the capability
  to learn. Like the brain, these neurons are connected to each other. The
  exact structure of neural networks and how these connections and neurons work
  will be discussed in the following pages. 
</p>

<p>
  So why are neural networks so popular?  In recent years, neural networks have
  exploded in popularity.  Cheaper and more powerful hardware allowed thousands
  and then millions of neurons in artificial neural networks. This renaissance
  in machine learning has allowed countless breakthroughs in neural networks
  and their applications. The second reason is the versatility of neural
  networks. Neural networks have found applications in text, speech, images and
  much more. They are a useful machine learning tool in a variety of
  situations. 
</p>

<p>
  There is a good amount of math associated with neural networks. This post
  will go into much of the surface level theory that goes into a neural
  network. From this theory you should have a good understanding of how basic
  neural networks work and can dive into actual implementation.
</p>


<!-- Part 1 -->
<h2>Neuron Theory</h2>

<p>
  What is a neural network? Let's begin by dissecting the name itself.
  A network can be though of as a graph and is nothing more than a set of
  vertices or <i>nodes</i> that are connected via edges. An example of a
  network is shown below. 
</p>

<img class='center-image' src='/assets/img/ml/crash_course/directed_acyclic_graph.png' />

<p>
    Each of the circles with a number in it corresponds to a node. Note that
    the edges connecting nodes have a one way relation. Say you wanted to get
    from node 7 to node 10. You would start at node 7, go to 11, then 10.
    However, going from node 10 to 7 would be impossible. We will see that
    neural networks are represented in much the same manner as a directed graph
    where each node represents a neuron with edges connecting neurons together.
</p>

<p>
    The other important aspect of our neural network is the neural part. We
    want our network to be able to understand and learn. While this might seem
    far fetched we will see that neural networks use these graphs as a powerful
    structure to define operations to learn desired responses. 
    Similar to the real historical beginnings of
    neural networks let's use inspirations of biology to get started. 
</p>

<p>
    Examine an individual node in our graph. Inspired by the brain we 
    will call this a neuron. An illustration of a neuron is shown below. 
</p>

<img class='center-image' src='/assets/img/ml/crash_course/single-neuron.png' />

<p>
  Before we get into the actual intuition of what a neuron is, let's get
  through the math behind one. 
</p>

<p>
    The input of the incoming edge is notated as the scalar \( p \). This edge has 
    a weight of the scalar \( w \). The weight is multiplied by the input to form the value 
    \( wp \). This is then sent into the the summation block which sums the 
    input \( wp \) and the bias \( b \). Notice that the bias has no 
    dependence on external input. Summing these two terms together then 
    gives \( wp + b \) or in the above image \( n \).
</p>

<p>
    The output of the summation \( n \) is then passed through the activation 
    function \( f \). The activation function is just some real valued scalar
    function. This then gives the final output of the neuron 
    \( f(n) = a \). The neuron output can then be calculated as 
    $$ a = f(wp + b) $$
</p>





<p>
  So what is the intuition behind a neuron? We can view the output of this
  neuron as making a <i>decision</i>.  This decision is based on the
  inputs, weights and bias of the neuron. Let's say you are trying to make the
  decision of if you want to go to a party tonight. Let's say we live in a
  world where this decision depends on only two factors: are you tired and is
  your best friend at the party? Note that these factors are simple yes or no
  questions. We can <i>encode</i> yes as \( 1 \) and no as \( 0 \). 
</p>

<p>
  The importance of these two factors will vary a lot from person to person.
  This corresponds to different weight values. For this neuron say that our
  activation function is the simple linear function \( f(x) = x \) where we say
  that any output \( > 0 \) means we should decide to go to the party and any
  output \( < 0 \) means that we should decide to not go. A normal person would
  not want to go to a party while tired. We should then make the weight (\(w_0
  \)) for the are you tired input (\( x_0 \)) be negative. Hopefully the
  opposite is true about your best friend, so the  weight (\(w_1\)) for that input
  (\(x_1\)) would be positive. 
</p>

<p>
  Say that you absolutely hate going out when you are tired and this is far
  more important than if your best friend is at the party. We could make \(
  w_0 = -10 \) and \( w_1 = 1 \) to represent this. If you  are tired you will
  never go out even if your best friend is there \( -10 + 1 < 0 \). However, if
  your best friend is there but you are not tired you would still go out \( 0 +
  1 > 0 \). If you were not tired and your best friend wasn't there you would
  be on the decision boundary and could just choose randomly.
</p>

<p>
  We can now incorperate bias to change our decision boundary. When we had no
  bias in the previous example the decision boundary was 0. If we set the bias
  to be \( 1 \) the decision boundary will be \( 1 \). A more positive decision
  boundary means we are less inclined to go to parties given any inputs. Let's
  change the problem slightly for \( x_1 \) to be the number of your friends
  that are going. If you generally enjoy going to parties your decision neuron
  could have \( b = -2 \) and of course you want your friends to be there and
  do not like going to a party while tired so \( w_0 = -4, w_1 = 1 \). So even
  if you are tired it would only take three of your friends to be there for you
  to want to go to the party. But if \( b = 0 \) it would take five friends if
  you are tired.
</p>

<p>
  In our example we choose the linear action function where our equation took
  the form \( a = w_0 x_0 + w_1 x_1 + b \). While we choose the linear action
  function there are a variety of other activation functions that are employed
  in neurons. Our simple linear linear activation function would look like the
  below for input \( p \) and output \( a \). 
</p>

<img class='center-image' src='/assets/img/ml/crash_course/pure-linear-transform.png' />

<p>
  Going back to the decision about the party, say there is another person that
  is trying to predict if you are going to go to the party. The neuron that
  represents this should have a probabilistic output so our same hard rule of if
  the output is greater than \( 1 \) will not apply. We could just take the
  pure value and based on how positive or negative it is determine how certain
  you are to go to the party. However, there is a function called the
  <i>sigmoid</i> function that does a better job of representing these
  probabilistic outputs. Any probability can be represented between 0 and 1.
  The sigmoid function does just this. Below is an image of the sigmoid
  function. 
</p>

<img class='center-image' src='/assets/img/ml/crash_course/sigmoid.png' />

<p>
  So very negative outputs will produce values close to 0 and very positive
  outputs will produce outputs close to 1. 
</p>

<p>
  Neural networks are probabilistic systems and therefore functions like the
  sigmoid function are a lot more powerful than just the linear activation
  function. We will see why the sigmoid function and other non-linear functions
  are so powerful in later lessons.
</p>

<p>
  Before moving on let's clean up some of the math behind what we have
  developed with the neuron so far. Say we have the multiple input neuron
  pictured below. 
</p>

<img class='center-image' src='/assets/img/ml/crash_course/multiple_input_neuron.png' />

<p>
  Each of the inputs to the node can just be represented as a vector to make
  the representation easier. 
  $$ \textbf{p} = \begin{bmatrix}
                    p_1 \\
                    p_2 \\
                    p_3 \\
                    \vdots \\
                    p_R
                  \end{bmatrix}$$
  Know that bold face represents a vector. 
</p>
<p>
  Likewise, we can also formulate the list of weights for each input value as a
  vector. 
  $$
  \textbf{w}_1 =  \begin{bmatrix}
                    w_{1,1},
                    w_{1,2},
                    w_{1,3} ,
                    \dots 
                    w_{1, R}
                  \end{bmatrix}
  $$
  Note that the weight vector is a row vector not a column vector this
  necessary for when we will have to multiply with the input vector. 
</p>

<p>
  Just as before we are simply multiplying the input by the weight. So our new
  vector would be just to multiply each input by the weight on that edge. 
  $$
  \begin{bmatrix}
    w_{1,1} p_1,
    w_{1,2} p_2,
    w_{1,3} p_3,
    \dots
    w_{1,R} p_R
  \end{bmatrix}
  $$
  This is the same as \( \textbf{w}_1 \textbf{p}\). 
</p>

<p>
  The next step is to go through the summation. Summing up the components of this
  vector gives 
  $$ w_{1,1} p_1 + w_{1,2} p_2 + w_{1,3} p_3 + \dots + w_{1, R} p_R $$
  Then we add in the bias \(b\) which as before is just a single scalar.
  $$ n = w_{1,1} p_1 + w_{1,2} p_2 + w_{1,3} p_3 + \dots + w_{1, R} p_R + b =
  \textbf{w}_1 \textbf{p} + b$$
</p>

<p>
  Note that \(n=\textbf{w}_1 \textbf{p}+b\) is still just a scalar. We then
  transform the input by the activation function to get the final output of the
  node. 
  $$ a = f(\textbf{w}_1 \textbf{p} + b )$$
</p>

<p>
  We know that the weights of a neuron control how a decision is made by the
  neuron. Different weights will give a neuron different decision properties.
  What if we wanted to work with multiple neurons each having different weights
  at the same time? We could do this by stacking the neurons into a
  <i>layer</i> of neurons where the inputs are feed into each neuron in
  parallel. 
</p>

<img class='center-image' src='/assets/img/ml/crash_course/neuron_layer.png' />

<p>
  Now the same principle applies as before. For node \(i\) calculate the output
  \(a_i\) through the following formula. Note that we are assuming that all of
  the activation functions are the same which as we will see later is a safe
  assumption to make for this case.
  $$ a_i = f(\textbf{w}_i \textbf{p} + b) $$
  However, we can simplify this and view the weights as a matrix of weights
  represented as follows saying there are \(S\) nodes that the input is being fed
  into.
  $$ \textbf{W} = 
  \begin{bmatrix}
    w_{1,1} & w_{1,2} & w_{1,3} & \dots & w_{1,R} \\
    w_{2,1} & w_{2,2} & w_{2,3} & \dots & w_{2,R} \\
    w_{3,1} & w_{3,2} & w_{3,3} & \dots & w_{3,R} \\
    \dots & \dots & \dots & \dots & \dots \\
    w_{S,1} & w_{S,2} & w_{S,3} & \dots & w_{S,R} \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    \textbf{w}_1 \\
    \textbf{w}_2 \\
    \textbf{w}_3 \\
    \vdots \\
    \textbf{w}_S \\
  \end{bmatrix}
  $$
  We also no longer have a single bias but now a bias for each neuron and there
  are \(S\) neurons
  $$
  \textbf{b} = \begin{bmatrix}
  b_1,
  b_2,
  b_3,
  \dots
  b_S
  \end{bmatrix}
  $$

  Likewise, we can say there is an output vector \(\textbf{a}\) that can be
  calculated through the following formula.
  $$ \textbf{a} = f(\textbf{W} \textbf{p} + \textbf{b})$$
</p>


<p>
  What happens if we feed the outputs of one layer of neurons into another
  layer of neurons? 
</p>

<img class='center-image' src='/assets/img/ml/crash_course/multiple_layers.png' />

<p>
  This is where we begin to see the power of neural networks. Each layer of
  neruon works on the abstraction of the previous layer. This allows deeper
  layers to make more complex and higher level decisions. Let's take a concrete
  example. Say you were builidng a neural network that takes as input
  handwritten images. The first layer could detect edges. The second could
  identify the contour the edges form. The third could take these contours and
  identify them with shapes. The fourth and final could take these shapes and
  associate them with numbers. 
</p>

<p>
  Now notate the weight matrix for layer \(i\) to be \(\textbf{W}^{i}\). Note
  that the number of neurons stacked vertically does not have to be the same in
  each layer. The neurons simply connect to all of the next level so it does
  not matter how neurons are in the previous layer for the current layer. The
  example below is a little more concrete of such a network.
</p>

<img class='center-image' src='/assets/img/ml/crash_course/network_example.png' />

<p>
  We can simply extend the rules used to compute the output of one layer and
  extend it to multiple layers. For instance to compute the output of the above
  network would be the following. 
  $$
  \textbf{a}^3 = f^3 ( \textbf{W}^3 f^2 ( \textbf{W}^2 f^1 (\textbf{W}^1
  \textbf{p} + \textbf{b}^1) + \textbf{b}^2) + \textbf{b}^3 )
  $$
  Notice how the input \(p\) is propagated from the left of the network to the
  right of the network.
</p>

<p>
  Typically you will see neural networks illustrated in the less expressive
  version shown below to save space. In the illustration each neuron is simply
  a node.
</p>

<img class='center-image' src='/assets/img/ml/crash_course/nn_illustration.png' />

<p>
  That is all for the basics of a neural network building blocks. You should be
  able to see how a neural network produces an output for some input. But what
  are all these transformations doing? The key is choosing the right values for
  the weights and the biases to make the network do interesting things. We can
  do this through having the network <b>learn</b> the weights and biases.
</p>

<p>
  We simply tell the neural network to learn, for some datset \(X,
  Y\), the mapping from \( X \) to \( Y \) and the network will find the
  appropriate weights to do so. 
</p>

<p>
  The neural network is a statistical learner. Given a set of input data \(X\)
  and a set of output data \( Y \) a neural network can learn the mapping from
  \( X \) to \( Y \). 
</p>


<!-- Part 2 --> 

<h2>Training a Neural Network</h2>

<p>
  Let's consider the simple neural network below that has two inputs to a
  single neuron.
</p>

<img class='center-image' src='/assets/img/ml/crash_course/adaline_two_input.png' />

<p>
  Now let's use our network to classify generic inputs 
  \(   \begin{bmatrix}
    \textbf{p}_1 \\
    \textbf{p}_2 
  \end{bmatrix} \). We can begin to construct this classifier by saying that
  for any output \( a > 0 \) the input can be categorized as class one and for any output \( a \le
  0\) the input can be categorized as class two. We can find the decision
  boundary by setting the equation of the network equal to zero. 
  $$ \textbf{wp} + b = 0 $$
  $$ w_1 p_1 + w_2 p_2 + b = 0$$
  $$ w_2 p_2 = - w_1 p_1 - b $$
  $$ p_2 = - \frac{w_1}{w_2} p_1 - \frac{b}{w_2} $$

  It should be clear that this is in the form \( y = mx + b \). Let's graph
  this equation.
</p>

<img class='center-image' src='/assets/img/ml/crash_course/adaline_decision.png' />

<p>
  All of the area covered in gray is for class one and all the white area is
  class two. This is a basic linear classifier. Think of it like linear
  regression but instead of continuous outputs its output is a category or
  class.
</p>

<p>
  We will now discover an algorithm that can be used to find the line to best
  separate data points. Take the example below where a line is discovered to
  best separate the blue and the red data points. We want to algorithmically
  find this line. 
</p>

<img class='center-image' src='/assets/img/ml/crash_course/linear_class.png' />

<p>
  Let's simplify how we represent this equation by putting all of the
  parameters that are to be adjusted in one term. Note that \( p_1 \) and \(
  p_2 \) cannot be adjusted but are given. Say that \( \textbf{z} =
  \begin{bmatrix}
    \textbf{w} \\
    b
  \end{bmatrix} \) and \( \textbf{x} = 
  \begin{bmatrix}
    \textbf{p} \\
    1
  \end{bmatrix} \) notice that the dot product of these two vectors gives the
  same result as before 

  $$ a = \textbf{x}^T \textbf{z} = \textbf{wp} + b $$

  Transposing the \( \textbf{x} \) term is to simply make the product of \(
  \textbf{x} \) and \( \textbf{z} \) possible to compute and while keeping both
  vectors as column vectors. Remember the rules of matrix multiplication!
</p>

<p>
  We can say that the error is the value we <i>expected</i> subtracted by the
  <i>actual</i> output of the network. Say \( t \) was the value expected and \( a \) was the
  output of the network. The error would then be \( e = t - a \). A common measure
  for error is the mean square error. This is given by the expected value of
  the squared error. The expected value here is the expected value of a given
  input given consideration of all input/output pairs. Mathematically we can
  write this as

  $$ J(\textbf{x}) = E[e^2] = E[(t-a)^2] = E[(t-\textbf{x}^T \textbf{z})^2] $$

  Expanding this expression then gives the following. Note that we can use the
  linearity of expected value to split the expected value terms apart.
  $$ J(\textbf{x}) = E[t^2 - 2 t \textbf{x}^T \textbf{z} + \textbf{x}^T \textbf{z} \textbf{z}^T \textbf{x} ] $$

  Note that in the last term all of the funky business done with the transposes is just to make
  the matrix multiplication work out. \( \textbf{x}^T \textbf{z}
  \textbf{x}^T \textbf{z} \) is not a valid product but \( \textbf{x}^T \textbf{z}
  \textbf{z}^T \textbf{x} \) is. Next apply use the linearity of expected
  value.

  $$ J(\textbf{x}) = E[t^2] - 2\textbf{x}^T E[t\textbf{z}] + \textbf{x}^T
  E[\textbf{z}\textbf{z}^T]\textbf{x} $$

  Let's just simplify the form a little bit by making the following
  substitutions.
  $$ J(\textbf{x}) = c - 2\textbf{x}^T\textbf{h} + \textbf{x}^T\textbf{Rx} $$
  where
  $$ c = E[t^2], \textbf{h} = E[t\textbf{z}], \textbf{R} = E[\textbf{z} \textbf{z}^T] $$
</p>

<p>
  It turns out this is in the form of a quadratic function. Finding the minimum
  point of this quadratic function will minimize the mean square error which is
  our goal. We can do this by finding the gradient of the function and
  setting it equal to \( 0 \).

  $$ \nabla J(\textbf{x}) = \nabla ( c - 2\textbf{x}^T\textbf{h} +
  \textbf{x}^T\textbf{Rx} ) = -2\textbf{h} + 2\textbf{Rx} = 0$$

  $$ \textbf{R} \textbf{x} = \textbf{h} $$
  $$ \textbf{R}^{-1} \textbf{R} \textbf{x} = \textbf{R}^{-1} \textbf{h} $$

  $$ \textbf{x} = \textbf{R}^{-1} \textbf{h} $$
</p>

<p>
  So \( \textbf{R}^{-1} \textbf{h} \) is the minimum point of our mean square
  error \( J(\textbf{x} ) \). However, in practice we do not actually compute
  these values \(\textbf{R}^{-1}\) and \(\textbf{h}\) but instead approximate
  the gradient of \( J(\textbf{x}) \) numerically. We can do so through the
  Least Mean Squares LMS algorithm. We will also shortly see that
  approximating this numerically will have a number of other benefits when
  expanding to more complex functions.
</p>

<p>
  The gradient of \( J(\textbf{x}) \) is given by:

  $$ J(\textbf{x}) = (t(k) - a(k))^2 = e^{2} (k) $$
  $$ \nabla J(\textbf{x}) = \nabla e^{2} (k) $$

  Where \( k \) is the iteration number. Instead of computing the expected
  value of the squared error we are approximating it at some iteration \( k \) 
  This approximation of the gradient is called stochastic gradient. And
  we descend the gradient looking for the minimum, giving rise to the important term in machine learning
  called <b>Stochastic Gradient Descent (SGD)</b>. Our derivatives are with respect to
  the network parameters, the bias and the weights. 
</p>

<p>
  As there are many weights for our neuron, the following expression gives the
  gradient of our term evaluated at the arbitrary weight \( j \). Say there are
  \( R \) inputs (and therefore weights) to this neuron we are considering.
  
  $$ [ \nabla e^2(k) ]_j = \frac{\partial e^2 (k)}{\partial w_j} $$

  We can then apply the chain rule.

  $$ \frac{\partial e^2 (k)}{\partial w_j} = 2 e(k) \frac{\partial e(k)}{\partial w_j} $$

  Do the same for the bias term, the other parameter of our network.

  $$ \frac{\partial e^2 (k)}{\partial b} = \frac{\partial e^2 (k)}{\partial b}
  = 2 e(k) \frac{\partial e(k)}{\partial b} $$
</p>  

<p>
  Now evaluate the term \( \frac{\partial e(k)}{\partial w_j} \) 

  $$ \frac{\partial e(k)}{\partial w_j} = \frac{\partial (t(k) - 
  a(k))}{\partial w_j}  = \frac{\partial}{\partial w_j} ( t(k) - (\textbf{w}^T
  \textbf{p}(k) + b))$$

  \(p_i(k)\) is \( i \)th element of the input at the \( k \)th iteration. 

  $$ \frac{\partial}{\partial w_j} \left( t(k) - \left( \sum_{i=1}^{R} w_i
  p_i(k) +b \right) \right) $$

  Many of the terms in this expression are not dependent on the network weights
  \( w_j \). \( \frac{\partial t(k) }{\partial w_j} = 0 \) and \(
  \frac{\partial b}{\partial w_j} = 0 \). Furthermore, \( \frac{\partial w_i
  p_i}{\partial w_j} = 0\) for any \(i \ne j\). The only nonzero term of this
  expression comes out to be \( \frac{\partial (w_j p_j)}{\partial w_j} = p_j \)
  but the negative sign in front of the summation makes \( -p_j \). We now have

  $$ \frac{\partial e(k)}{\partial w_j} = -p_j(k) $$

  When taking the derivative of the expression with respect to the bias \( b
  \), we can see the only term dependent on \( b \) is \( b \) itself. This
  just gives \( 1 \). 

  $$ \frac{\partial e(k)}{\partial b} = -1 $$
</p>

<p>
  To simplify writing the final formula let's go back to our previous notation
  of using \( \textbf{z} \) to represent the parameters of the network. 
  $$ 
  \textbf{z}(k) = 
  \begin{bmatrix}
    p_j(k) \\
    1
  \end{bmatrix} 
  $$

  We can now rewrite the original equation for the gradient of the squared
  error. 

  $$ \nabla J(\textbf{x}) = \nabla e^2 (k) = -2e(k)\textbf{z}(k) $$

  At iteration \( k \) all we have to do is multiply the input \( p_j \) by the
  error.
</p>

<p>
  Our next step is to "descend" this gradient to find the minimum points. We
  know that our gradient is in direction \( \nabla J(\textbf{x}) \) therefore,
  for a given point \( \textbf{x}_k \) the direction of the minimum point is \(
  \textbf{x}_k - \alpha \nabla J( \textbf{x}_k) \) where \( \alpha \) is how
  far we go down the gradient. So we can say that the next iteration should be
  the previous iteration after taking this step in the direction of the
  gradient. We can write this as follows.
</p>

<p>
  $$ \textbf{x}_{k+1} = \textbf{x}_k + 2 \alpha e(k) \textbf{z}(k) $$

  We say that \( \alpha \) is the learning rate. 
  This can then be split up into the weight and bias update terms.

  $$ \textbf{w}(k+1) = \textbf{w}(k) + 2\alpha e(k) \textbf{p}(k) $$
  $$ b(k+1) = b(k) + 2\alpha e(k) $$
</p>

<p>
  Let's now write that equation for the general case when there could be many
  neurons in the layer such as with the image below. Note that the activation
  function \( f \) in this case is the linear activation function.
</p>

<img class='center-image' src='/assets/img/ml/crash_course/neuron_layer.png' />

<p>
  Simply look at a given "row" \( i \).

  $$ \textbf{w}_{i}(k+1) = \textbf{w}_{i}(k) + 2\alpha e_{i}(k) \textbf{p}(k) $$
  $$ b_{i}(k+1) = b_{i}(k) + 2\alpha e_{i}(k) $$

  Where \( e_{i} \) is the error of the \( i \)th row of the error in the
  layer's output. And then we can just write it in matrix form to clean things
  up. 

  $$ \textbf{W}(k+1) = \textbf{W}(k) + 2\alpha \textbf{e}(k) \textbf{p}(k) $$
  $$ \textbf{b}(k+1) = \textbf{b}(k) + 2\alpha \textbf{e}(k) $$
</p>

<p>
  And that's all we need to train our network. Using this algorithm the network
  would be able to converge to optimal network parameters. However, we are limited to
  single layer networks and linear activation functions. In the next section we
  will examine how to generalize this to any number of layers and any
  activation function. 
</p>


<h2>Sources</h2>
<ul>
  <li>
    <a href='http://neuralnetworksanddeeplearning.com'>neuralnetworksanddeeplearning.com</a>
  </li>
  <li>
    <a href='http://hagan.okstate.edu/NNDesign.pdf'>
      Neural Network Design by Hagan
    </a>
  </li>
  <li>
    <a href='https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618'>
      Deep Learning (Adaptive Computation and Machine Learning Series) by
      Goodfellow, Bengio, Courville
    </a> 
  </li>
  <li>
    <a href='http://cs231n.github.io/'>CS231n Convolutional Neural Networks for
    Visual Recognition Class Notes</a>
  </li>
  <li>
    <a href='https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/'> 
      Why You Should Use Cross-Entropy Error Instead Of Classification Error Or Mean Squared Error For Neural Network Classifier Training by James D.  McCaffery
    </a>
  </li>
</ul>
