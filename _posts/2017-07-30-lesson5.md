---
layout: post
published: true
title: "Lesson 5: Neural Networks, Part 2 (Training)"
headline: "One step forward, one step back"
mathjax: true
featured: true
categories: curriculum

comments: true
---

### Table of Contents
0. [Introduction](#introduction)
1. [Gradient Descent](#gd)
2. [Backpropagation](#bp)
3. [Loss Functions](#loss)
4. [Optimization in Practice](#opt)
5. [Regularization](#reg)
6. [The Unstable Gradient Problem](#ugp)



<h2><a name="introduction"></a>Introduction</h2>

In the last lesson, we learned how neural networks can apply a series of transformations on some input data in order to eventually produce a final prediction. This process of transforming the input data by feeding it through layers of artificial neurons, of each of which applies its own weights, biases, and activation functions, is known as "Forward Propagation."

However, you'll note that we never actually went through our training data and used it to train our model parameters (i.e. the neuron weights and biases). As a result, the transformations that were being applied to the input data were pretty much random, since we randomly initiated the weights and biases. As you could probably predict, this would mean that our final output would be more or less random as well.

In this lesson, you'll see how we can start from this random initialization, use a cost function to measure just how bad our predictions are, and gradually train our network to make more and more meaningful predictions via gradient descent. However, since these neural networks have more than one layer, you'll see that we have to do some extra work in order to calculate these gradient terms for our model parameters.

Once we've gone through the work of training out network, the end result will be a flexible model that is capable of learning even more complex relationships within data than linear regression or logistic regression.

<h2><a name='gd'></a>Gradient Descent</h2>

To see how exactly we can train neural networks to make predictions that match
the training data, we'll continue with our example of trying to predict test
scores based hours studied and hours slept.  Let's start off with a video
discussing some ideas that should feel pretty familiar to you from the lesson
on linear regression: cost functions and gradient descent.

<p style="text-align:center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/5u0jaA3qAGk" frameborder="0" allowfullscreen></iframe>
</p>

**Recap:** Just like we did in linear regression, we'll set up a cost function that will measure just how bad (costly) our predictions are. This will primarily be a function of our model parameters (weights and biases), since we can get different cost values by changing our parameters. When we say that we are "training a network," we really mean that we finding the parameters that minimize our cost function.

To do this, we will use gradient descent to find out how, given our current parameter values, we can tweak our parameters so that we reduce our overall cost. Just as before, we will repeatedly apply these gradient descent steps until we converge on our optimal parameters.

**Notes:** That Yann LeCun guy that the speaker in the video mentions is widely considered to be something of a machine learning god by the AI community. You'll see his name pop up a whole lot in our next lesson on Convolutional Neural Networks.


<h2><a name='bp'></a>Backpropagation</h2>

Next, we'll see how to actually calculate our gradient terms so that we know
which way to move our parameters. While this was relatively straightforward in
linear regression, it will take some more work when it comes to multi-layer
neural networks. While watching the next video, feel free to pause and think
whenever necessary. It will likely be helpful to think about what each symbol
actually means in English. Here are some good starting points to get your feet
planted:

  * $$W^{(i)}$$ is the weight matrix used by the $$i$$th layer in the network. These are the parameters that we eventually want to be able to optimize.
  * $$z^{(i)}$$ is a vector containing the weighted sums of the inputs (and bias terms) of the neurons in the $$i$$th layer. This term directly depends on the weights of this layer, since these weights will dictate the value of the weighted sum.
  * $$a^{(i)}$$ is a vector containing the *activations* of the neurons of the $$i$$th layer, and is equal to the weighted sum of the layer after passing it through the activation function. This term directly depends on the weighted sum $$z^{(i)}$$.
  * And a new one: $$\delta^{(i)}$$ is a vector representing the "error" of each of the individual neurons in the $$i$$th layer. Mathematically, it is given by $$\frac{\partial J}{\partial z^{(i)}}$$, where $$z^{(i)}$$ is the weighted sum of the neurons within layer in question.

We will start by finding the error terms at the right-most end of the network
(the neurons closest to the output), and then use the (multivariate) chain rule
to propagate this error back through the previous layers until we calculated
have the gradient terms for all our model weights. You can think of this
process as tracking how changing one neuron's weight somewhere in the network
affects that neuron's activation, which in turn affects all the next layer's
activations (since they all take in all the previous layer's activations as
inputs), which eventually affects the final cost. We want to find out how we
can change that one weight so that it shifts the following layers' activations
in a way that reduces our final cost.

<p style="text-align:center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/GlcnxUlrtek" frameborder="0" allowfullscreen></iframe>
</p>

**Recap**: In order to actually calculate our gradient terms $$\frac{\partial J}{\partial w_{ij}}$$, we need to be pretty clever in the way that we track how changing one weight anywhere in the network ends up affecting the final cost. To do this, we'll work our way backward through the network: we'll start by calculating the error terms at the neurons closest to the output, and then keep propagating this error through to the earlier layers. (This will require us to apply the chain rule a pretty absurd number of times.) Along the way, we'll see which weights are responsible for most of the error in the network, and then adjust their gradient terms accordingly.

As we propagate the error backward, we'll eventually begin to reveal which way we should change the weights early on in our network so that they change the subsequent activations in a positive way (that is, in a way that reduces our final cost). Once we have all our gradient terms calculated, we'll finally know which way to apply our gradient descent step so that we end up decreasing our cost function, and making our predictions just a little more accurate.

**Notes**: In case this has you lost, don't worry. Almost everyone feels lost when they learn about backpropagation the first couple times around. If you'd like to try looking at another source, check out [**this section**](http://neuralnetworksanddeeplearning.com/chap2.html) on backpropagation from Mike Nielson's online book, Neural Networks and Deep Learning. You may find his explanations more intuitive than most.

<h3>Backprop: Written Explanation</h3>

<p>
  The example in the video was only for a two layer network. Here, we will
  generalize the backpropagation algorithm to an arbitrary neural network. To see a detailed derivation of these
  equations, visit <a
  href='/blog/curriculum-supplement/lesson5supplement'><b>the supplemental
  lesson we have put together</b></a>.
</p>

<p>
  Below, we will go through the basic equations behind backpropagation. 
  For the simplicity's sake, we'll assume that the activation function is the
  same throughout the entire network. However, you should see that this could
  easily be generalized to different activation functions for each layer. 

  <br><br>

  Let's say that we have a neural network with \(\textbf{L}\) layers, input \(\textbf{x}\) and a single activation function \( f \) that is used by each layer. 

  <br><br>

  Just as in the video, we'll use \( \delta \) to refer to the "error" of a given neuron. 
  For any individual neuron, this error term
  tells us how changing the weighted sum of the neuron (\( z \)) changes the overall cost of the network.
  Since each layer we're dealing with will have multiple neurons, 
  we'll use \( \delta^m \) as a vector to represent the error terms of each
  of the neurons in the \( m \)th layer.

  <br><br>

  (In some other textbooks/resources, you may see these error
  terms referred to as "sensitivities", and notated with \( s^m \).)

  <br><br>

  Recall these basic equations that we use for forward propagation:

  <br>

  $$
  \textbf{a}^{0} = \textbf{x} \text{  (Starting with the input layer)}
  $$

  $$
  \textbf{z}^{m+1} = \textbf{W}^{m+1}\textbf{a}^{m} + \textbf{b}^{m+1}
  \text{  (Computing the weighted sum from any one layer to the next)}
  $$

  $$
  \textbf{a}^{m+1} = f(\textbf{z}^{m+1})
  \text{  (Passing the weighted sum through an activation function)}
  $$

  $$
  \textbf{a} = \textbf{a}^{L}
  \text{  (Using the output of the last layer as out final output)}
  $$

  Given this output, let's backpropogate the error terms to adjust the 
  parameters of the network based
  on the expected output \( \textbf{t} \). Since changes in the earlier layers
  will subsequently affect all later layers, we'll have to use the chain rule to
  work backward from the final (output) layer to the earlier layers. 

  <br><br>

  To compute the error of the neurons in the final layer, we use the chain rule to calculate:

  $$
  \delta^{L} = \frac{\partial J}{\partial \textbf{Z}^L} = 
  \frac{\partial \textbf{a}^L}{\partial \textbf{Z}^L} \frac{\partial J}{\partial \textbf{a}^L} = 
  f'(\textbf{z}^L) * \frac{\partial J}{\partial \textbf{a}}
  $$

  In this equation, \(f' = \frac{\partial f}{\partial \text{z}^m} \), or the derivative of the activation function with respect to its input (i.e. the weighted sum).

  <br><br>

  Then, we have to backpropagate this error to all the earlier layers.
  The main challenge here is to write out the error of one layer (\( \delta^m \)) in terms of the 
  error of the next layer (\( \delta^{m+1} \)). Once we have this relationship, we can start
  working backward through the network to infer how changing the
  weights of one layer early on in the network will trickle down through
  all the layers of the network, and eventually affect the final cost.

  $$
  \delta^{m} = \frac{\partial J}{\partial \textbf{Z}^m} = 
  \frac{\partial \textbf{a}^m}{\partial \textbf{Z}^m} \frac{\partial \textbf{z}^{m+1}}{\partial \textbf{a}^m}
  \frac{\partial J}{\partial \textbf{z}^{m+1}} = 

  f'(\textbf{z}^{m}) \left( \textbf{W}^{m + 1} \right)^{T}
  \delta^{m+1}
  $$

  To work backward through the network, we repeat these calculations for \( m = L - 1 \) to \( m = 0 \),
  until we have finally calculated the error terms for every layer in the network. 

  <br><br>

  Then, to get from the error to the actual gradient in a given layer, we can take:

  $$
  \frac{\partial J}{\partial \textbf{W}^{m}} = 
  \frac{\partial J}{\partial \textbf{z}^{m}} \frac{\partial \textbf{z}^m}{\partial \textbf{W}^{m}} =
  \delta^{m} (\textbf{a}^{m-1})^{T}
  $$


  Using these error terms, we can now update
  the parameters of the network. 

  We'll use \( \alpha \) as our learning rate to
  control how quickly we descend our gradient, and \( k \) to denote our current
  gradient descent iteration.

  $$ \textbf{W}^{m}(k+1) = \textbf{W}^{m}(k) - \alpha \delta^{m}
  (\textbf{a}^{m-1})^{T} $$

  $$ \textbf{b}^{m}(k+1) = \textbf{b}^{m}(k) - \alpha \delta^{m} $$
</p>

<p>
  Using these equations, we can now train any arbitrary neural network
  to learn some function from input to output. Getting to an acceptable
  level of accuracy will typically take
  several iterations of gradient descent over the entire input set (each full
  iteration is known as an <i>epoch</i>).
</p>

<p>
  As a final clarification, backpropagation is the step of propagating the error terms
  backwards in the network. Gradient descent is the process of actually 
  using these error terms to update the
  parameters of the network.
</p>

<h2><a name="loss"></a>Loss Functions</h2>

<p>
  In our equations of backpropagation, we have used a generalized loss function \( J
  \). This cost function is what we aim to through stochastic
  gradient descent (SGD) and tells us how good the model is doing given the model 
  parameters.
</p>

<p>
  In earlier lessons, we saw the mean squared error used as a loss function. 
  However, the mean squared
  error may not always be the best cost function to use. In fact, a more popular loss
  function is the <i>cross entropy cost function.</i> Before we get
  more into the cross-entropy cost function, let's look into the <i>softmax
  classification function</i>.
</p>

<h3><a name="softmax"></a>Softmax Classifier</h3>

<p>
  Let's say you are building a neural network to classify between two classes.
  Our neural network will look something like the following image. Notice
  that there are two outputs \( y_1 \) and \( y_2 \) representing class one and
  two respectively.
</p>

<img class='center-image' src='/assets/img/ml/crash_course/decision_network.png' />

<p>
  We are given a set of data points \( \textbf{X} \) and their corresponding
  labels \( \textbf{Y} \). How might we represent the labels? A given point is
  either class one or class two. The boundary is distinct. If you remember the
  linear classification boundary from earlier we said that any output greater
  than 0 was class one and any output less than 0 was class two. However, that
  does not really work here. A given data point \( \textbf{x}_i \) is simply
  class one or class two. We should not have data points be more class one than
  other data points.
</p>

<p>
  We will use <i>one hot encoding</i> to provide labels for these points. If a
  data point has the label of class one simply assign it the label vector \(
  \textbf{y}_i =
  \begin{bmatrix}
    1 \\
    0
  \end{bmatrix} \) and for a data point of class two assign it the label vector
  \( \textbf{y}_i = \begin{bmatrix}
    0 \\
    1
  \end{bmatrix} \)
</p>

<p>
  Say our network outputs the value \( \begin{bmatrix}
    c_1 \\
    c_2
  \end{bmatrix} \) where \(c_1, c_2 \) are just constants. We can say the
  network classified the input as class one if \( c_1 > c_2 \) or classified as
  class two if \( c_2 > c_1 \). Let's use the softmax function to interpret
  these results in a more probabilistic manner.
</p>

<p>
  The softmax function is defined as the following
  $$
    q(\textbf{c}) = \frac{e^{c_i}}{\sum_j e^{c_j}}
  $$

  Where \( c_i \) is the scalar output of the \(ith\) element of the output
  vector. Think of the numerator as converting the output to an un-normalized
  probability. Think of the denominator as normalizing the probability. This
  means that for every output \( i \) the loss function will have an output
  between 0 and 1. Another convenient property of the softmax function is that 
  the sum of all the output probabiilties \( i \) will sum to \( 1 \),
  making this activation function more suitable to working with probability distributions.
</p>

<h3>Entropy</h3>

<p>
  We need to take one more step before we can use the softmax function as a loss function. This
  requires some knowledge of what <i>entropy</i> is. Think about this example. Say you
  were having a meal at EVK, one of the USC dining halls. If your meal is bad,
  this event does not carry much new information, as the meals are almost guaranteed
  to be bad at EVK. However, if the meal is good, this event carries a lot of
  information, since it is out of the ordinary. You would not tell anyone about the bad meal
  (since a bad meal is pretty much expected), but you would tell everyone about the good meal.
  Entropy deals with this measure of information. If we know an underlying
  distribution \( y \) to some
  system, we can define how much information is encoded in each event. We can
  write this mathematically as:
  $$
    H(y) = \sum_i y_i \log \left( \frac{1}{y_i} \right) = - \sum_i y_i \log (
    y_i )
  $$
</p>

<h3>Cross Entropy</h3>

<p>
  This definition assumes that we are operating under the correct underlying
  probability distribution. Let's say a new student at USC has no idea what the
  dining hall food is like and thinks EVK normally serves great food. This
  freshman has not been around long enough to know the true probability
  distribution of EVK food, and instead assumes the probability
  distribution \( y'_i \). Now, this freshman incorrectly
  thinks that bad meals are uncommon. If the freshman were to tell a
  sophomore (who knows the real distribution) that his meal at EVK was
  bad, this information would mean little to the sophomore because the
  sophomore already knows that EVK food is almost always bad. We can say that the cross
  entropy is the encoding of events in \( y \) using the wrong probability
  distribution \( y' \). This gives
  $$
  H(y, y') = - \sum_i y_i \log y'_i
  $$
</p>

<p>
  Now let's go back to our neural network classification problem. We know the
  true probability distribution for any sample should be just the one hot
  encoded label of the sample. We also know that our generated probability
  distribution is the softmax function. This gives the final form of our cross
  entropy loss.
  $$
  L_i = -\log \left( \frac{e^{c_i}}{\sum_j e^{c_j}} \right)
  $$
  Where \( y_i = 1 \) for the correct label and \( y' \) is the softmax
  function.
  This loss function is often called the categorical cross entropy loss
  function because it works with categorical data (i.e. data that can be
  classified into distinct classes).
</p>

<p>
  And while we will not go over it here, know that this function has calculable
  derivatives as well. This allows it to be used just the same as the mean
  squared error loss function. However, the cross
  entropy loss function has many desirable properties that the mean squared
  error does not have when it comes to classification.
</p>

<p>
  Let's say you are trying to predict the classes cat or dog. Your neural
  network has a softmax function on the output layer (as it should because this
  is a classification problem). Let's say for two inputs \(
  \textbf{x}_1,\textbf{x}_2 \) the network respectively outputs
  $$
  \textbf{a}_1 =
  \begin{bmatrix}
    0.55 \\
    0.45
  \end{bmatrix},
  \textbf{a}_2 =
  \begin{bmatrix}
    0.44 \\
    0.56
  \end{bmatrix}
  $$
  where the corresponding labels are
  $$
  \textbf{y}_1 =
  \begin{bmatrix}
    1 \\
    0
  \end{bmatrix},
  \textbf{y}_2 =
  \begin{bmatrix}
    0 \\
    1
  \end{bmatrix}
  $$

  As you can see, the network only barely classified each result as correct. But
  by only looking at the classification error, the accuracy would have been
  100%.
</p>

<p>
  Take a similar example where the output of the network is just slightly off.
  $$
  \textbf{a}_1 =
  \begin{bmatrix}
    0.51 \\
    0.49
  \end{bmatrix},
  \textbf{a}_2 =
  \begin{bmatrix}
    0.41 \\
    0.59
  \end{bmatrix},
  \textbf{y}_1 =
  \begin{bmatrix}
    0 \\
    1
  \end{bmatrix},
  \textbf{y}_2 =
  \begin{bmatrix}
    1 \\
    0
  \end{bmatrix}
  $$

  Now in this case, we would have a 0% classification accuracy.
</p>

<p>
  Let's see what our cross entropy function would have given us in each
  situation when averaged across the two samples.

  In the first situation:
  $$
  -(\log(0.55) + \log(0.56)) / 2 = 0.59
  $$

  In the second situation:
  $$
  -(\log(0.49) + \log(0.59)) / 2 = 0.62
  $$

  Clearly, this result makes a lot more sense for our situation than just
  having a cost value of \( 0 \) for a barely-correct classification.
</p>

<p>
  Overall, the choice of the correct loss function is dependent on the problem, and is a
  decision you must make in designing your neural network. Always keep
  in mind the general equations for stochastic gradient descent will have the form:
  $$
  \mathbf{W} (k) = \textbf{W}(k-1) - \alpha \nabla J(\textbf{x}_k, \textbf{W}(k-1))
  $$
  $$
  \mathbf{b} (k) = \textbf{b}(k-1) - \alpha \nabla J(\textbf{x}_k, \textbf{b}(k-1))
  $$
  Where \( J \) is the loss function. Furthermore, the same form of
  backpropagation equations will still apply with backpropagating the
  sensitivities through the network.
</p>


<h2><a name='opt'></a>Optimization in Practice</h2>

Watch the following video for an introduction to further methods in optimization.

<p style="text-align:center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/9KM9Td6RVgQ" frameborder="0" allowfullscreen></iframe>
</p>

**Recap:** Although gradient descent is a super powerful tool, it isn't without its flaws. Turns out, it may not always converge on the optimal parameters for our model: it may end up overshooting the optimal point, finding a local minimum of our cost function instead of the global minimum, etc. Thankfully, there's a whole slew of optimization techniques that build upon gradient descent to allow our network to more intelligently minimize the cost function.

<h3>Mini-batch Algorithm</h3>

<p>
  First let's review our SGD algorithm shown below. Note that \( \theta \) is
  commonly used to refer to all the parameters of our network (including weights and biases).
  $$
  \mathbf{\theta} (k) = \mathbf{\theta}(k-1) - \alpha \nabla J(\textbf{x}_k,
  \mathbf{\theta}(k-1))
  $$
  As this algorithm is <i>stochastic</i> gradient descent, it operates on one input
  example at a time. This is also referred to as online training. However, this
  is not an accurate representation of the gradient, as it is only over a single
  training input, and is not necessarily reflective of the gradient over the
  entire input space. A more accurate representation of the gradient could be
  given by the following.
  $$
  \mathbf{\theta} (k) = \mathbf{\theta}(k-1) - \alpha \nabla J(\textbf{x},
  \mathbf{\theta}(k-1))
  $$
  The gradient at each iteration is now being computed across the entire input
  space. This is referred to as <i>batch gradient descent</i>, which 
  actually turns out to be a kind of confusing name in practice (as we'll see in a bit).
</p>

<p>
  In practice, neither of these approaches are desirable. The first does not
  give a good enough of an approximation of the gradient -- the second is
  computationally infeasible, since for each iteration, the gradient of the cost
  function for the entire dataset has to be computed. <b>Mini-batch</b> methods
  are the solution to this problem.
</p>

<p>
  In mini-batch training, a sample set of all the training examples are used to compute the cost
  gradient. The average of these gradients for each sample is then used. This
  approach offers a good trade off between speed and accuracy. The equation
  for this method is given below, where \( Q \) is the number of samples in the
  mini-batch and \( \alpha \) is the learning rate.
  $$
  \mathbf{\theta} (k) = \mathbf{\theta}(k-1) - \frac{\alpha}{Q} \sum_{q=1}^{Q}\nabla
  J(\textbf{x}_q, \mathbf{\theta}(k-1))
  $$
  Remember that batch gradient descent is over the whole input space, while
  mini-batch is just over a smaller subset at a time.
</p>

<p>
  Of course, it would make sense that the samples have to be randomly drawn from
  the input space, as sequential samples will likely have some correlation. The
  typical mini-batch sampling procedure is to randomly shuffle the input space, and then 
  to sample sequentially from the scrambled inputs.
</p>

<h3>Initializations</h3>

<p>
  At this point, you may be wondering how the parameters of a neural network are typically
  initialized. So far, the learning procedure has been described, but the actual initial
  state of the network has not been discussed.
</p>

<p>
  You may think that how a network is initialized does not necessarily matter.
  After all, the network should eventually converge to the correct parameters
  right? Unfortunately, this is not the case with neural networks; as it turns out, the
  initialization of the parameters matters greatly. Initializing to small random weights
  typically works. But typically, the standard for weight initialization is
  the normalized initialization method.
</p>

<p>
  In the normalized initialization method, weights are randomly drawn from the following uniform
  distribution:
  $$
  \textbf{W} \sim U \left( -\frac{6}{\sqrt{m+n}}, \frac{6}{m+n} \right)
  $$
  Where \( m \) is the number of inputs into the layer and \( n \) is the
  number of outputs from the layer.
</p>

<p>
  As for the biases, typically just assigning them to a value of 0 works.
</p>

<h3>Challenges in Optimization</h3>

<p>
  In mathematical optimization, we optimize (i.e. find the minimum/maximum of)
  some function \( f \). In machine learning, we can think of training a neural network
  as a specific type of optimization (gradient descent) applied to a specific cost function.
</p>

<p>
  As you may have expected, there are some concerns that can arise in this training phase: 
  for example, local minima. Any deep neural network is guaranteed to
  have a very large number of local minima. Take a look at the below
  surface. This surface has two minima: one local, and one global. If you look at
  the contour map below, you can see that the algorithm converges to the local
  minimum instead of the global minimum.
</p>

<img class='center-image' src='/assets/img/ml/crash_course/optimization_surface_minima.png' />

<p>
  
  Should we take measures to stop our neural network from converging 
  at a local (rather than a global) minimum?
  Local minima would be a concern if the
  cost function evaluated at the local minima was far greater than the cost
  function evaluated at the global minima. It turns out that in practice,
  this difference is often
  negligible. Most of the time, simply finding any minima is sufficient in the
  case of deep neural networks.
</p>

<p>
  Some other potential problem points are saddle points, plateaus, and valleys. In practice, neural
  networks can often escape valleys or saddle points. However, they can still pose a
  serious threat to neural networks, since they can have cost values much
  greater than at the global minimum. Even more dangerous to the training
  process are flat regions on the cost surface. Small initial
  weights are chosen in part to avoid these flat regions.
</p>

<p>
  In general, more flat areas are problematic for the rate of convergence. It
  takes a lot of iterations for the gradient descent algorithm to get over flatter regions.
  One's first thought may be to increase the learning rate of the algorithm, but
  too high of a learning rate will result in divergence at steeper areas of the
  performance surface. When this algorithm with a high learning rate goes
  across something like a valley, it will oscillate out of control and diverge.
  An example of this is shown below.
</p>

<img class='center-image' src='/assets/img/ml/crash_course/momentum.png' />

<p>
  At this point, it should be clear that several modifications to
  backpropagation need to be made to allow solve this oscillation problem and
  to fix the learning rate issue.
</p>

<h3>Momentum</h3>

<p>
  For this concept, it is useful to think of the progress of the algorithm
  as a point traveling over the cost surface. Momentum in neural
  networks is very much like momentum in physics. And since our 'particle'
  traveling over the cost surface has unit mass, momentum is just the
  velocity of our motion. The equation of backprop with momentum is given by the
  following.

  $$
  \textbf{v}(k) = \lambda \textbf{v}(k-1) - \alpha \nabla J(\textbf{x}, \mathbf{\theta}(k-1))
  $$
  $$
  \mathbf{\theta} (k) = \mathbf{\theta}(k-1) + \textbf{v}(k)
  $$

  The effect of applying this can be seen in the image below. Momentum dampens
  the oscillations and tends to make the trajectory continue in the same
  direction. Values of \( \lambda \) closer to 1 give the trajectory more momentum.
  Keep in mind that \( \lambda \) itself does not actually represent the magnitude 
  of the particle's momentum;
  instead, it is more like a force of
  friction for the particle's trajectory. Typical values for \( \lambda \) are 0.5,
  0.9, 0.95 and 0.99.
</p>

<img class='center-image' src='/assets/img/ml/crash_course/momentum_working.png' />

<p>
  Nesterov momentum is an improvement on the standard momentum algorithm. With
  Nesterov momentum, the gradient of the cost function is considered after the
  momentum has been applied to the network parameters at that iteration. So now
  we have:
  $$
  \textbf{v}(k) = \lambda \textbf{v}(k-1) - \alpha \nabla J(\textbf{x},
  \mathbf{\theta}(k-1) + \lambda \textbf{v}(k-1))
  $$
  $$
  \mathbf{\theta} (k) = \mathbf{\theta}(k-1) + \textbf{v}(k)
  $$
  In general, Nesterov momentum outperforms standard momentum.
</p>

<h3>Adaptive Learning Rates</h3>

<p>
  One of the most difficult hyper-parameters to adjust in neural networks is the
  learning rate. Take a look at the image below to see the effect of learning
  different learning rates on the minimization of the loss function.
</p>

<img class='center-image' src='/assets/img/ml/crash_course/learningrates.jpeg' />

<p>
  As from above, we know that the trajectory of the algorithm over flat sections
  of the cost surface can be very slow. It would be nice if the
  algorithm could have a fast learning rate over these sections, but a slower
  learning rate over steeper and more sensitive sections. Furthermore, the
  direction of the trajectory is more sensitive in some directions as opposed
  to others. The following algorithms will address all of these issues with
  adaptive learning rates.
</p>

<h3>AdaGrad</h3>

<p>
  The Adaptive Gradient algorithm (AdaGrad) adjusts the learning rate of each
  network parameter according to the history of the gradient with respect to
  that network parameter. This is an inverse relationship, so if a given network
  parameter has had large gradients (i.e. steep slopes) in the recent past, the learning rate will
  scale down significantly.
</p>

<p>
  Whereas before there was just one global learning rate, there is now a per
  parameter learning rate. We set the vector \( \textbf{r} \) to be the
  accumulation of the parameter's past gradients, squared. We initialize this term
  to zero.
  $$
  \textbf{r} = 0
  $$

  Next we compute the gradient as usual
  $$
  \textbf{g} = \frac{1}{Q} \sum_{q=1}^{Q}\nabla J(\textbf{x}_q, \mathbf{\theta}(k-1))
  $$

  And then accumulate this gradient in \( r \) to represent the history of the
  gradient.

  $$
  \textbf{r} = \textbf{r} + \textbf{g}^2
  $$

  And finally, we compute the parameter update
  $$
  \mathbf{\theta} (k) = \mathbf{\theta}(k-1) - \frac{\alpha}{\delta +
  \sqrt{\textbf{r}}}
  \odot g
  $$

  Where \( \alpha \) is the global learning rate, and \( \delta \) is an extremely
  small constant ( \( 10^{-7} \) ). Notice that an element wise vector
  multiplication is being performed (by the \( \odot \) operator). Remember that each element of the gradient
  represents the partial derivative of the function with respect to a given
  parameter. The element wise multiplication will then scale the gradient with
  respect to a given parameter appropriately. The global learning rate is usually not
  difficult to choose, and normally works as just 0.01.
</p>

<p>
  However, a problem with this algorithm is that it considers the whole
  sum of the squared gradient since the <b>beginning</b> of training. In practice,
  This results in
  the learning rate decreasing too much too early.
</p>

<h3>RMSProp</h3>

<p>
  RMSProp is regarded as the go-to optimization algorithm for deep neural
  networks. It is similar to AdaGrad, but includes a decay over the
  accumulation of the past gradient-squared, so the algorithm "forgets" gradients far
  in the past.
</p>

<p>
  As normal, compute the gradient.
  $$
  \textbf{g} = \frac{1}{Q} \sum_{q=1}^{Q}\nabla J(\textbf{x}_q, \mathbf{\theta}(k-1))
  $$

  Now, this is where the algorithm changes with the introduction of the decay
  term \( \rho \), which is set somewhere between 0 and 1.

  $$
  \textbf{r} = \rho \textbf{r} + (1 - \rho) \textbf{g}^2
  $$

  And the parameter update is the same.
  $$
  \mathbf{\theta} (k) = \mathbf{\theta}(k-1) - \frac{\alpha}{\delta +
  \sqrt{\textbf{r}}}
  \odot g
  $$
</p>

<h3>Second Order Algorithms</h3>

<p>
  Second order algorithms make use of the second derivative to "jump" to the
  critical points of the cost function. Further discussion of these algorithms
  is outside the scope of this tutorial. However, these algorithms do not work
  very well in practice. First of all, it is computationally infeasible to
  compute the second order derivatives. Second of all, for a complex
  performance surface with many critical points, it is very likely the second
  order method would go in the completely wrong direction. Overall, gradient
  descent first order methods have been shown to perform
  better, so I would not worry about knowing what second order algorithms are
  all about. But know that they exist and are an active area of research.
</p>



<h2><a name="reg"></a>Regularization</h2>

<p>
  The following video builds off the last video and introduces the concept of
  regularization.
</p>

<p style="text-align:center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/S4ZUwgesjS8" frameborder="0" allowfullscreen></iframe>
</p>

**Recap:** Data is never perfect: our observations contain both signal (which reflects some real, underlying process), and random noise. If we make the model fit our observations too closely, we will end up fitting it to this noise as well, which will make for some pretty weird and unintuitive predictions.

In order to diagnose overfitting, we'll hide a part of our observed data from the model (we'll call this the *test set*), and only train our model based on the remaining data (we'll call this the *training set*). Measuring our model's performance on the test set will allow us to get an unbiased view of how well our model can make generalized predictions from unseen data, so if our test set cost is much higher than our training set cost, we can be fairly certain that our model has overfit the training data at the expense of generalized prediction ability.

Potential fixes for overfitting include getting more data (if possible), and using a technique called *regularization*. Regularization penalizes overly complex models by adding the squared-magnitudes of the model weights to the cost function, often resulting in smoother, more *robust* models that are more capable of making general predictions in the real world.


<p>
  When we design a machine learning algorithm, the goal is to have the algorithm
  to perform well on unseen inputs. Regularization deals with this, 
  allowing the model to 
  perform
  well on the test set which the algorithm has never seen before, sometimes at
  the cost of the training accuracy. Regularization is the process of putting
  a penalty terms in the cost function to help the model generalize to new
  inputs. Regularization does this by controlling the complexity of the model
  and preventing overfitting.
</p>

<p>
  Given a cost function \( J(\theta, \textbf{X}, \textbf{y}) \) we can write
  the regularized version as follows. (Remember \( \theta \) notates the
  parameters of the model).
  $$
  \hat{J}(\theta, \textbf{X}, \textbf{y}) = J(\theta, \textbf{X}, \textbf{y}) +
  \alpha \Omega(\theta)
  $$

  The \( \Omega \) term is the parameter norm penalty and operates on the
  parameters of the network. The constant \( \alpha \in [0, \infty) \) controls
  the effect of the regularization on the cost function. This is a
  hyperparameter that must be tuned. Also another note, when we refer to the
  parameters of the model in regularization we typically only refer to the
  weights of the network not the biases.
</p>

<h3>\(L^2\) Parameter Regularization</h3>

<p>
  This type of regularization defines the parameter norm penalty as the
  following.
  $$
  \Omega(\theta) = \frac{1}{2} \lVert \textbf{w} \rVert _2^2
  $$
  and the total objective function:
  $$
  \hat{J}(\theta, \textbf{X}, \textbf{y}) = J(\theta, \textbf{X}, \textbf{y}) +
  \frac{\alpha}{2} \textbf{w}^T \textbf{w}
  $$

  Evidently, this regularization will penalize larger weights. In theory, this
  should help prevent the model from overfitting. It is common to employ \( L^2
  \) regularization when the number of observations is less than the number of
  features. Similar to \( L^2 \) regularization is \( L^1 \), which you can
  probably expect is just \( \Omega(\theta) = \frac{1}{2} \lVert \textbf{w}
  \rVert _1 \). In almost all cases, \( L^2 \) regularization outperforms \( L^1
  \) regularization.
</p>

<h3>Early Stopping</h3>

<p>
  When we are working with a dataset, we split that dataset up into testing and
  training datasets. The training dataset is used to adjust the weights of the
  network. The test dataset is used to check the accuracy of the model on data that
  has never been seen before.
</p>

<p>
  However, the training dataset can be divided again into the training data and
  a small subset of data called the validation set. The validation set is used
  during training to ensure that the model is not overfitting. (You can think of the validation
  set as lying somewhere between the training set, which is used at every step of the
  training process,
  and the test set, which is only used at the very end of the training process to
  evaluate the model's generalizability.)

  This validation data is not ever used
  to train the model. The validation accuracy refers to the model's accuracy
  over the validation set. The goal is to minimize the validation accuracy
  through tuning hyperparameters of the network. The network is only evaluated
  on the test dataset with the fully tuned model.
</p>

<p>
  Take a look at the below graph showing validation loss versus training loss.
  It should be clear that at a certain point, the model overfits on the training
  data and begins to suffer in validation accuracy despite this not being
  reflected in the training accuracy.
</p>

<img class='center-image' src='/assets/img/ml/crash_course/valid_train_loss.png' />

<p>
  The solution to this is to simply stop training once the validation set loss
  has not improved for some time. Just like \( L^1 \) and \( L^2 \)
  regularization, this is a method of decreasing overfitting on the training
  dataset.
</p>

<h3><a name="ensemble"></a>Ensemble Methods</h3>
<p>
  <i>Bagging</i> (short for bootstrap aggregation, a term in statistics) is the
  technique of making a model generalize better by combining multiple weaker
  learners into a stronger learner. Using this technique, several models are
  trained separately and their results are averaged for the final result. This
  ideal of one model being composed of several independent models is called an
  ensemble method. Ensemble methods are a great way to fine tune your model to
  make it generalize better on test data. Ensemble methods apply to more than
  just neural networks, and can be used on any machine learning technique.
  Almost all machine learning competitions are won using ensemble methods.
  Often times, these ensembles can be comprised of dozens and dozens of
  learners.
</p>

<p>
  The idea is that if each model is trained independently of each other, they
  will have their own errors on the test set. However, when the results of the
  ensemble learners are averaged the error should approach zero.
</p>

<p>
  Using bagging, we can even train a multiple models on the same dataset but be
  sure that the models were trained independently. With bagging, \( k \)
  different datasets of the same size are constructed from the original dataset
  for \( k \) learners. Each dataset is constructed by sampling from the
  original dataset with some probability with replacement. So there will be
  duplicate and missing values in the constructed dataset.
</p>

<p>
  Furthermore, differences in model initialization and hyperparameter tuning
  can make ensembles of neural networks particularly favorable.
</p>

<h3>Dropout</h3>

<p>
  Dropout is a very useful form of regularization when used on deep neural
  networks. At a high level, dropout can be thought of randomly removing neurons
  from some layer of the network with a probability \( p \). Removing certain
  neurons helps prevent the network from overfitting.
</p>

<p>
  In reality, dropout is a form of ensemble learning. Dropout trains an ensemble
  of networks where various neurons have been removed and then averages the
  results, just as before. Below is an image that may help visualize what
  dropout does to a network.
</p>

<img class='center-image' src='/assets/img/ml/crash_course/dropout.jpeg' />

<p>
  Dropout can be applied to input units and hidden units. The hyperparameter of
  dropout at a given layer is the probability with which a neuron is dropped.
  Furthermore, another major benefit of dropout is that the computational cost
  of using it is relatively low. Finding the correct probability will require
  parameter tuning because a probability too low and dropout will have no
  effect, while too high and the network will be unable to learn anything.
</p>

<p>
  Overall, dropout makes more robust models and is a standard technique
  employed in deep neural networks.
</p>

<h2><a name="ugp"></a>The Unstable Gradient Problem</h2>

<p>
  As a final section, let's go over a problem in optimization that plagued the
  deep learning community for decades.
</p>

<p>
  As we've seen, deeper neural networks can be a lot more powerful than their
  shallow counterparts. Deeper layers of
  neurons add more layers of abstraction for the network to work with. Deep
  neural networks are vital to visual recognition problems. Modern deep neural
  networks built for visual recognition are hundreds of layers deep.
</p>

<p>
  However, you may think that you can take what you have learned so far build a
  very deep neural network and expect it to work. However, to your surprise you
  may see that adding more layers at a certain point does not seem to help and
  even reduces the accuracy. Why is this the case?
</p>

<p>
  The answer is in unstable gradients. This problem plagued deep learning up
  until 2012, and its relatively recent solutions are responsible 
  for much of the deep learning boom. The cause
  of the unstable gradient problem can be formulated as different layers in the
  neural network having vastly different learning rates. And this problem only
  gets worse with the more layers that are added. The vanishing gradient
  problem occurs when earlier layers learn slower than later layers. The
  exploding gradient problem is the opposite. Both of these issues deal with
  how the errors are backpropagated through the network.
</p>

<!-- TODO: Change notation, vocab (Use: "error", delta) -->

<p>
  Let's recall the equation for backpropagating the error terms through the network.
  $$
  \delta^{m} =
  f'(\textbf{z}^{m}) \left( \textbf{W}^{m + 1} \right)^{T}
  \delta^{m+1}
  $$

  Now let's say the network has 5 layers. Let's compute the various
  sensitivities recursively through the network.

  $$
  \delta^5 = \frac{\partial J}{\partial \textbf{z}^5}
  $$

  $$
  \delta^4 = f'(\textbf{z}^4)(\textbf{W}^{5})^T \frac{\partial J}{\partial \textbf{z}^5}
  $$

  $$
  \delta^3 = f'(\textbf{z}^3)(\textbf{W}^{4})^T f'(\textbf{z}^4)(\textbf{W}^{5})^T \frac{\partial J}{\partial \textbf{z}^5}
  $$

  $$
  \delta^2 = f'(\textbf{z}^2)(\textbf{W}^{3})^T f'(\textbf{z}^3)(\textbf{W}^{4})^T f'(\textbf{z}^4)(\textbf{W}^{5})^T \frac{\partial J}{\partial \textbf{z}^5}
  $$

  $$
  \delta^1 = f'(\textbf{z}^1)(\textbf{W}^{2})^T f'(\textbf{z}^2)(\textbf{W}^{3})^T f'(\textbf{z}^3)(\textbf{W}^{4})^T f'(\textbf{z}^4)(\textbf{W}^{5})^T \frac{\partial J}{\partial \textbf{n}^5}
  $$

  The term for \( \delta^1 \) is massive, and this is only for a five layer
  deep network. Imagine what it would be for a 100 layer deep network! The
  important take away is that all of the terms are being multiplied together in a giant chain.
</p>  

<p>
  For a while, the sigmoid function was believed to be a powerful activation
  function. The sigmoid function and its derivative are shown below.
</p>

<img class='center-image' src='/assets/img/ml/crash_course/derivative_sigmoid.png' />

<p>
  Say we were using the sigmoid function for our five layer neural
  network.  That would mean that \( f' \) is the function
  shown in red. What is the maximum value of that function? It's around 0.25.
  What types of values are we starting with for the weights? Small random
  values. The key here is that the values start small. The cause of the vanishing gradient
  problem should now start becoming clear. Because of the chain rule, we are
  recursively multiplying by terms less far less than one, causing the
  sensitivities to shrink and shrink going backwards in the network.  
</p>

<p>
  With this many multiplication terms, it would be something of a magical balancing act
  to manage all the terms so that the overall expression does not explode or
  shrink significantly.
</p>

<p>
  How do we fix this problem? The answer is actually pretty simple. Just use
  the ReLU activation function instead of the sigmoid activation function. The
  ReLU function and its derivative are shown below.
</p>

<img class='center-image' src='/assets/img/ml/crash_course/relu.png' />

<p>
  As you can see, its derivative is either 0 or 1, which alleviates the unstable
  gradient problem. This function is also much easier to compute.
</p>


## Conclusion

If you made it through these past two lessons, you should hopefully have a good idea now of how the architecture of a neural network is set up, and how neural networks use their layers of neurons to gradually transform input data into a final prediction. (Remember: multiply inputs by weights, add it all up -- including a bias, apply an activation function, and repeat.)

You should hopefully also have a general notion of how we go about training these neural networks, and also an idea of what challenges can arise during the training process. (For example, vanishing gradients, local minima, overfitting, etc.)

If some of the topics brought up in these last two lessons still don't quite make sense to you, that's perfectly ok. This material is pretty difficult for most people to pick up at first, but only gets better with patience and practice. If you have any questions, feel free to shoot us an email at <a href='mailto:caisplus@usc.edu'>caisplus@usc.edu</a> and we will do our best to get back to you as soon as we can.

Otherwise, stay tuned for the next lesson, in which we'll use the theoretical basis developed in these lessons to proceed onto a more powerful type of model: the Convolutional Neural Network. Convolutional Neural Networks are a specific type of neural network designed for processing and understanding images, and are still at the forefront of machine learning research.

Now that you've built up a basic machine learning foundation, it's time to let the real fun begin!

<br>

<h2>Sources</h2>
<ul>
  <li><a href='http://neuralnetworksanddeeplearning.com'>neuralnetworksanddeeplearning.com</a></li>
  <li>
    <a href='http://hagan.okstate.edu/NNDesign.pdf'>
      Neural Network Design by Hagan
    </a>
  </li>
  <li>
    <a href='https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618'>
      Deep Learning (Adaptive Computation and Machine Learning Series) by
      Goodfellow, Bengio, Courville
    </a>
  </li>
  <li>
    <a href='http://cs231n.github.io/'>CS231n Convolutional Neural Networks for
    Visual Recognition Class Notes</a>
  </li>
  <li>
    <a
    href='https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/'>
      Why You Should Use Cross-Entropy Error Instead Of Classification Error Or
      Mean Squared Error For Neural Network Classifier Training by James D.
      McCaffery
    </a>
  </li>
</ul>
