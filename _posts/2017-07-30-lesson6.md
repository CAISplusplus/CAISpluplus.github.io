---
layout: post
published: true
title: "Lesson 6: Convolutional Neural Networks"
headline: ""
mathjax: true
featured: true
categories: curriculum 

comments: false
---

<h2>Introduction</h2>

<p>
  Before reading this lesson I would highly recommend first checking out the
  lesson on <a href='/blog/machine_learning/neural_networks/introduction'>regular neural networks</a>. The topics in this lesson will build off
  of that lesson. 
</p>
<p>
  At this point we have learned how artificial neural networks can take in
  some numerical features as input, feed this input through a variety of
  transformations involving weights, biases and activation functions, and
  output a meaningful final result. We also learned how these neural networks
  can, given a number of training examples, gradually teach themselves to
  produce more accurate outputs, allowing them to optimize their performance
  without having to be manually programmed.
</p>

<p>
  In learning all this, we laid down the building blocks for almost all of
  modern deep learning. Turns out, the flexibility of these artificial neural
  networks allows us to develop even more sophisticated models specialized for
  certain tasks, often with much higher levels of performance and scalability.
  In this lesson, we’ll introduce one such specialized neural network created
  mainly for the task of image processing: the convolutional neural network.
</p>

<p>
  Convolutional neural networks <b>(abbreviated as CNNs or ConvNets)</b> are one of the the driving
  factors behind the deep learning learning movement today. CNNs have been
  proven to be <b>vastly</b> superior to traditional approaches in the realm of
  images and other spatial data. The next several lessons will lay the
  theoritical framework for the motivation and implementation of CNNs and how
  they power much of the modern deep learning frenzy. 
</p>

<h2>Motivation</h2>

<p>
  Let's say that we are trying to build a neural network that takes
  <i>images</i> as input. To do so we would simply feed each pixel of the image
  into the neural network. To do so we would 'flatten' the image into a vector
  so that the neural network could understand it. A 200x200 image would then be
  a 40,000 dimension vector.  The input layer of our network would have to have
  a weight for each pixel of the image.  Furthermore, color images will need a
  weight for each color component of the image (Think RBG spectrum. Any color
  can be expressed in terms of some combination of red, blue and green).  For a
  200x200 RBG image this would be \(200*200*3 = 120,000\) weights. For just the
  first layer! And 200x200 is not even that big of an image! To get any decent
  results we would have to add many more layers easily resulting in millions of
  weights all of which need to be learned.
</p>

<p>
  This approach is obviously not scalable. This is why CNNs were created. CNNs
  are built to work with spatial data. This could be images, video, sound or
  even in some cases text.
</p>

<p>
  CNNs are very similar to ordinary Neural Networks from the previous chapter.
  They are made up of layers of neurons, each with a set of learnable
  parameters that get adjusted over time. Each neuron receives some inputs,
  multiplies the inputs by its weights, adds on a bias term, and optionally
  follows it with an activation function. The whole
  network still learns from a dataset of training examples through
  backpropagation and gradient descent, using a single differentiable loss
  function at the end. Furthermore, almost all the tips/tricks we developed for
  learning regular neural networks still apply.
</p>

<p>
  Here is a quote from Yann LeCun the inventor of the CNN and director of AI
  research at Facebook about a high level description of CNNs. 
</p>

<p>
  <i>
  “A ConvNet is a particular way to connect the units in a neural net inspired
  by the architecture of the visual cortex in animals and humans. Modern
  ConvNets may utilize anywhere from seven to 100 layers of units [to process
  its input]. To a computer, an image is simply an array of numbers. Within
  this array of numbers, local motifs, such as the edge of an object, are
  easily detectable in the first layer. The next layer would detect
  combinations of these simple motifs that form simple shapes, like the wheel
  of a car or the eyes in a face. The next layer will detect combinations of
  shapes that form parts of objects, like a face, a leg, or the wing of an
  airplane. The last layer would detect combinations of parts that form
  objects: a car, an airplane, a person, a dog, etc. The depth of the network —
  with its multiple layers -- is what allows it to recognize complex patterns
  in this hierarchical fashion.”
  </i>
  - Yann LeCun
</p>

<p>
  So what makes CNNs so equipped to work with spatial data? Data is feed into
  convolutional neural networks as an array. for instance a 32x32 RBG image
  would be feed into the network as a 32x32x3 array. Mathematically we refer to
  this high dimensional construct not as a matrix or vector but a
  <b>tensor</b>. Working with this volumetric data requires special layers
  called convolution layers.
</p>


<h2>Convolution Layers</h2>

<p>
  First of all what does convolution even mean? The convolution is a
  mathematical operation used to extract features from an image. The
  convolution is defined by an image kernel. The image kernel is nothing more
  than a small matrix. For instance, a 3x3 kernel matrix is very common.
  Remember that we can think of an image as a array of numbers. For simplicity
  let's just look at greyscale images at first. 
</p>

<p>
  For instance, say this is our image. (Where we have 1 being black and 0 being
  white).
</p>

<img class='center-image' src='/assets/img/ml/cnn/image.png' />

<p>
  And this is our 3x3 kernel. 
</p>

<img class='center-image' src='/assets/img/ml/cnn/kernel.png' />

<p>
  The kernel is applied to the image by sliding it over the image and for each
  position computing the element wise multiplication of the two matrices and
  then summing up the result.
</p>

<img class='center-image' src='/assets/img/ml/cnn/kernresult.gif' />

<p>
  A different kernel will obviously produce a different output. The kernel can
  be specially selected to perform certain operations on the output image such
  as edge detection, sharpening or blurring. Below is an example of an edge
  detection kernel of \( 
  \begin{bmatrix}
    -1 & -1 & -1 \\
    -1 & 8 & -1 \\
    -1 & -1 & -1
  \end{bmatrix}
  \) applied to an image.
</p>

<img class='center-image' src='/assets/img/ml/cnn/edgedetectresult.jpg' />

<p>
  Now we can use multiple kernels to extract different features from an image.
  A demonstration of this is shown below. First the red kernel is slid over the
  input image to extract a feature map (just the result of the convolution) and
  next a different kernel colored in green does the same to produce a different
  feature map. 
</p>

<img class='center-image' src='/assets/img/ml/cnn/featuremap.gif' />

<p>
  A large part of the reason why the convolution operator is such a powerful
  feature detector is that it is space invariant. Since the kernel slides over
  the entire image it does not matter where in the image a given "feature" may
  be, the kernel will find it regardless. Furthermore, the kernel is applied
  uniformly (the values of the kernel do not change as it is slid over the
  image) so it will detect a feature the same in one part of an image as
  another. 
</p>

<p>
  Still confused about the convolution operator? Check out <a
  href='http://setosa.io/ev/image-kernels/'>this incredible demo</a> for an
  interactive way to explore convolution.
</p>

<p>
  The core idea of the CNN is to learn the values of these filters through
  backpropagation and to stack multiple layers of feature detectors (kernels)
  on top of each other for abstracted levels of feature detection. 
</p>

<h4>A Formal Definition of Convolution</h4>

<p>
  To gain a more in depth understanding of what convolution is let's take a
  look at it through a more rigorous definition. Convolution is a mathematical
  operation that has roots in signal processing. Say we are measuring the
  noisy signal \( x(t) \). Since this signal is noisy we can get a more
  accurate measurement by averaging the signal with the last several signals.
  However, more recent measurements are more relevant to the current position
  and we therefore want to have some sort of decaying weight function that
  penalizes measurements that happened a long time ago. Say \( w(a) \) is our
  weight function where \( a \) is the age of the measurement. Now say we
  wanted to get a measurement at time \( t \) taking into account this
  weighting. To do so we would have to apply the weighted average at every
  continuous moment in time before \( t \)
  $$
  s(t) = \int_{0}^{t} x(a) w(t-a) da
  $$
  This operation is the definition of convolution and is often notated as \(
  s(t) = (x*w)(t) \) The convolution function can be thought as the amount of
  overlap of functions \( x \) and \( w \). In the below image the green curve
  is the value of the convolution \( f * g \), the red is \( f \), the blue \(
  g \) and the shaded area is the product \( f(a) g(t - a) \) where \( t \) is
  the x-axis. 
</p>

<img class='center-image' src='/assets/img/ml/cnn/convgaus.gif' />

<p>
  The first argument to the convolution (in the example the function \(x(t)\))
  is the input to the function and the second (in the example the function \(
  w(t) \) ) is referred to as the kernel. 
</p>

<p>
  This continuous representation of convolution does not work for computers
  that only work with discrete values. We can convert the convolution to its
  discrete counterpart.
  $$
  s(t) = (x*w)(t) = \sum_{a=-\infty}^{\infty} x(a)w(t-a)
  $$
  However, remember that our goal is to apply this to images which have defined
  boundaries so we can constrain these infinite sums to the dimensions of the
  image.
</p>

<p>
  Furthermore, images are two dimensional so we must apply the convolution to
  a two dimensional function. 
  $$
  S(i, j) = (I * K)(i, j) = \sum_{m} \sum_{n} I(m,n) K(i - m, j - n)
  $$
  Get used to the notation of calling \( I \) as the input image and \( K \) as
  the kernel. Furthermore, in the above equation \( n \) and \( m \) would be
  clamped to the dimensions of the image.
</p>

<h4>Sparse Connections</h4>

<p>
  Put the convolution operator on the backburner for a little while because
  first we have to visit the next major breakthrough with convolutional neural
  networks: sparse connectivity. 
</p>

<p>
  In traditional neural networks each neuron is connected to every neuron in
  the next level. If a given layer has \( m \) inputs and \( n \) outputs the
  runtime of the matrix multiplication needed to compute the layer output is \(
  O( m * n) \). The connections of a fully connected layer are shown below. 
</p>

<img class='center-image' src='/assets/img/ml/cnn/full_connection.png' />

<p>
  However, say we limit the number of connections each node has to \( k \). The
  runtime of such an approach is now \( O(k * n) \) where \( k \) is orders of
  magnitude smaller than \( m \)
</p>

<img class='center-image' src='/assets/img/ml/cnn/sparse_connection.png' />

<h4>Convolution Layer</h4>

<p>
  The convolution layer is at the core of a CNN. Recall that a standard neural
  network layer takes a vector as input. A CNN takes a 
  <i>volume</i> as input. We will see how this concept makes
  sense soon. The neurons of a convolution layer are likewise arranged as
  volumes. The weights of a convolution layer can be viewed as a set of filters
  (remember from the convolution operator).
</p>

<img class='center-image' src='/assets/img/ml/cnn/cnn_vols.jpeg' />

<p>
  There are two primary concepts that make the convolution layer possible. The
  first is local connections. Just as explained above this makes only parts of
  the input connected to the neurons. In terms of convolutional layers this
  means that only patches of the input layer are connected to a given filter.
  The spatial extent of the weights connections to the input is called the
  receptive field or the filter size. The connectivity is always full along the
  depth axis. So in summary, connections are local in width and height, while full in
  the depth dimension. 
</p>

<p>
  Say an input volume is 32x32x3, meaning it is a 32x32 RGB image. We say that
  this image has 3 depth slices each of which have an area of 32x32 (where each
  depth slice corresponds to one of the color channels). If we choose a filter
  size of 5x5 in this situation then each neuron in the convolution layer would
  be connected to \( 5*5*3 = 75 \) weights (not including bias).  Note that
  this 5x5 filter size had to be connected to the full extent of the depth. 
</p>

<p>
  As we will see shortly it makes sense to call the incoming weights of the
  neuron a filter. 
</p>

<p>
  So how many neurons in the convolution layer are needed and what local
  patches are these neurons connected to? The hyper parameters of depth, stride and
  zero padding address all of this.
</p>

<p>
  Depth is the the number of neurons looking at the same local region of input.
  This will control the depth of the output volume. As each neuron can be
  viewed as a filter this controls the number of filters in the layer.
  Furthermore, each of these filters will be trained to detect something else
  on the same input region. 
</p>

<p>
  Stride specifies how to slide a filter over the input region. This is what
  connects one filter to the input region. For instance if the stride is 2
  the filter will move 2 pixels at a time. Furthermore, this can be viewed as a
  form of downsampling because higher strides will produce smaller output
  volumes. 
</p>

<p>
  Zero padding is simply padding the input image with 0's. So a zero padding of
  3 would add 3 layers of zero valued pixels around the borders. In practice
  zero padding gives a way to control the output volume without having to
  choose between smaller spatial extent or smaller kernels. 
</p>

<p>
  So the output volume size is dependent on the input volume size \( W \), the
  filter size \( F \) of the neurons and the stride \( S \) and the amount of
  zero padding \( P \). The output volume height and width is given by the following formula. 
  $$
  \frac{W - F + 2P}{S} + 1
  $$
  So for a 32x32 input and a 5x5 filter with a stride of 1 and zero padding of
  0 we would get a 28x28 output. Of course, fractional pixel values do not make
  sense so there are some combinations of \(W, F, P\) and \( S \) that do not work.
</p>

<p>
  The depth of the output layer is controlled by the depth of the neurons. We
  can notate this parameter as \( K \). 
</p>

<p>
  Now say that the filter size is 11, a stride of 4, depth is 96 and no zero
  padding. Say the input volume has a size of 227x227x3. Using the above
  formula we can deduce that the output of the first layer will have dimensions
  of 55x55x96. All 96 neurons along a depth column are connected to the same
  input region of dimension 11x11x3. 
</p>

<p>
  Now in the above example we would have a total of 55*55*96 = 290,400 neurons
  in just the first layer with each neuron having 11*11*3 weights (and 1 bias).
  Together this adds up to over 100 million parameters. This number can be
  reduced through <i>parameter sharing</i>. Parameter sharing makes the
  assumption that it is useful for a filter to calculate the same features
  across the entire region of a depth slice. This is a good assumption to make
  in most cases as remember this neural network will be many layers deep and
  therefore will have multiple layers of abstraction. We can use this
  assumption by making a single depth slice share the same weights and biases.
  So now instead of 55*55*96 neurons there are only 96 neurons. And as before
  each of those 96 neurons is connected to a 11*11*3 volume giving \(
  96*11*11*3 = 34,848 \) parameters (not including biases). 
</p>

<p>
  Now if all the neurons in a single depth slice are using the same weights
  then computing the forward pass of a convolution layer is the same as
  doing the convolution of the neuron's weights and the input volume. This
  is why we refer to the sets of weights for each neuron to be a filter or a
  kernel. 
</p>

<p>
   The same rules that applied to regular neural networks also apply to
   CNNs. The same concepts of backpropagation still apply and the network is
   still composed of numerous layers and a loss function. Let's take a closer
   look at what these filters are computing. Below is an image of 96 filters in
   the first layer of a convolutional neural network used in image detection. 
</p>

<img class='center-image' src='/assets/img/ml/cnn/filters.png' />

<p>
  Remember by the assumption of parameter sharing, we are assuming that if a
  filter learns that detecting a horizontal edge at some location in the image
  is important it will be important at all locations of the image. This is a
  fair assumption as typically images are invariant to translation. (i.e. if a
  picture of a cat has a cat in the top left hand corner, it is still a picture
  of a cat if the cat is moved to the opposite corner). 
</p>

<p>
  In my opinion the most valuable resource for really getting a solid grasp on
  how the whole process work is the convolution demo made for the Stanford 231n
  course. A quick note before checking out the demo, this demo is for when there are
  two filters of size 3, a stride of 2, and a zero-padding of 1. Note that the
  filter weight matrices are both composed of 3 3x3 matrices because the input
  volume has a depth of 3. I would recommend spending a lot of time studying
  the demo and getting a good grasp on what is happening. <a
  href='https://cs231n.github.io/assets/conv-demo/index.html'>Please find the
  demo here</a>. 
</p>

<p>
  While the convolution layer is certianly the driving innovation behind CNNs
  there is one more layer that is typically used in CNN architectures. Read the
  next section to find out more!
</p>

<h2>Pooling Layers</h2>

<p>
  The role of pooling layers is to reduce the dimensionality of the input. 
  Pooling layers are typically put right after convolution layers. Reducing the
  dimensionality of the input both reduces the number of parameters and
  therefore the complexity of the network and in turns reduces overfitting. 
</p>

<p>
  The pooling layer operates over every depth slice and does nothing to change
  the depth of the input. However, the output height and width are reduced
  through sampling areas of the image. 
</p>

<p>
  The most common type of pooling is max pooling. Max pooling takes only the
  maximum pixel value from a region of pixels. So for instance for a max
  pooling with filter size of 2x2 with a stride of 2 would reduce a 8x8 image
  to a 2x2 image. Typically the stride of the filter is just the filter size.
  In the image below each color of the image is a region that the pooling
  considers independently of the other regions. 
</p>

<img class='center-image' src='/assets/img/ml/cnn/pooling.png' />

<p>
  Max pooling actually makes the network a more powerful feature detector as
  only the areas with maximum response to the convolution operator are kept.
  This preserves the most important features while throwing out the less
  important features.
</p>

<p>
  Pooling can be viewed as a type of down sampling. For max pooling
  backpropagation just has to know which pixel from each region had the highest
  value in the forward pass so that the gradient is only routed through that
  pixel. However, besides that everything else remains the same. 
</p>

<p>
  And with that we have all the building blocks to a convolutional neural
  network!
</p>

<h2>Applications</h2>

<h4>Fully Connected Layers</h4>

<p>
  Before getting into the actual architecture of CNNs we first have to know
  that convolutional layers alone cannot make up a neural network that outputs
  a vector (which we need for classification and regression). A CNN outputs a
  3D volume which is a tensor, not a vector. 
</p>

<p>
  The basic architecture of a CNN is to stack successive layers of convolution and max
  pooling. However, after several convolution, max pooling pairs the image has
  been sufficiently processed to a low dimension, meaningful signal. 
</p>

<p>
  At this point it makes sense to make some sort of decision based on the
  signal produced by all of the convolution layers.  At this point the output
  of the convolution or max pooling layer is <i>flattened</i> and passed
  through several fully connected layers. These fully connected layers are just
  the basic neural network connections presented in the <a
  href='/blog/machine_learning/neural_networks/introduction'>previous</a>
  tutorial on neural network basics. Two or three of these fully connected
  layers followed by an output layer is common. 
</p>

<p>
  This fully connected network at the end of the CNN is referred to as the
  decision network. This is because it takes the signal from the convolution layers and
  makes a meaningful decision from it. 
</p>

<h4>Common Architectures</h4>

<p>
  Now let's take a look at actual CNN architectures. The development of CNNs
  have been a driving force behind the development of deep learning. As you
  will see CNNs gradually got deeper and deeper over history. Deeper CNNs are
  more powerful object detectors on high resolution images. This is only a sample of
  the CNN architectures that are out there. New architectures are coming out
  every day. 
</p>

<ul>
  <li>
    <b>LeNet-5</b>: This network is the original CNN. It is often used as a
    basic example for the MNIST dataset which is comprised of 32x32x1 images of
    handwritten digits (grey scale digits). The network architecture is in the following order:
    <ul>
      <li>
        Convolutional layer composed of 6
        5x5 filters. This reduces the input to 28x28x6.
      </li>
      <li>
        Downsampled by maxpooling that has a kernel size of 4 which gives an
        volume size of 14x14x6  
      </li>
      <li>
        Convolution layer with 16 filters for 10x10x16. 
      </li>
      <li>
        Max pooling layer reducing output to 5x5x16
      </li>
      <li>
        Flatten output
      </li>
      <li>
        Fully connected layer of 120 neurons
      </li>
      <li>
        Fully connected layer of 84 neurons
      </li>
      <li>
        Fully connected layer of 10 neurons
      </li>
      <li>
        Softmax function to determine class scores. (This was in the context of
        digit recognition and there are 10 digits)
      </li>
    </ul>
    A diagram of the network can be found below.

    <img class='center-image' src='/assets/img/ml/cnn/lenet5.png' />
  </li>
  <li>
    <b>AlexNet:</b> This network has a very similar architecture to LeNet-5 so
    I will not go into the exact details. The first 5 layers are convolution
    followed by max pooling then the last 3 layers are fully connected layers.
  </li>
  <li>
    <b>VGGNet:</b>  The network architecture is listed below. Note that C-64
    means convolution layer with 64 filters, P-2 means pooling layer with
    filter size of 2 and FC-4096 means full connected layer with 4096 neurons. 
    Notice that this network follows up a block of two or
    sometimes three convolution layers with a max pooling layer.
    <ul>
      <li>Input: [224x224x3]</li>
      <li>C-64: [224x224x64]</li>
      <li>C-64: [224x224x64]</li>
      <li>P-2: [112x112x64]</li>
      <li>C-128: [112x112x128]</li>
      <li>C-128: [112x112x128]</li>
      <li>P-2: [56x56x128]</li>
      <li>C-256: [56x56x256]</li>
      <li>C-256: [56x56x256]</li>
      <li>C-256: [56x56x256]</li>
      <li>P-2: [28x28x256]</li>
      <li>C-512: [28x28x512]</li>
      <li>C-512: [28x28x512]</li>
      <li>C-512: [28x28x512]</li>
      <li>P-2: [14x14x512]</li>
      <li>C-512: [14x14x512]</li>
      <li>C-512: [14x14x512]</li>
      <li>C-512: [14x14x512]</li>
      <li>P-2: [7x7x512]</li>
      <li>Flatten</li>
      <li>FC-4096: [4096]</li>
      <li>FC-4096: [4096]</li>
      <li>FC-1000: [1000]</li>
      <li>Softmax for class probabilities</li>
    </ul>
    Below is a visualization of VGGNet

    <img class='center-image' src='/assets/img/ml/cnn/vggnet.jpg' />

    <p>
      This network was a step in the direction of very deep neural networks. It
      showed that depth was an important property for the power of neural
      networks. 
    </p>

    <p>
      However, after this network researchers struggled to come up with deep
      networks that could maintain the original features of the images. This
      lead to the innovation of residual neural networks that featured skip
      connections to allow networks to preserve original image features.
    </p>
  </li>

  <li>
    <b>ResNet</b>: This architecture takes advantage of residual or skip
    connections and allows for much deeper neural networks. Typically ResNets of
    56 layers deep are employed. 
  </li>

  <li>
    <b>Inception v3</b>: Google's inception v3 network is a very deep neural
    network that is currently (or as of writing this) state of the art in CNN
    architectures. The network has 25 million trainable parameters and does 5
    billion multiply-adds for a single forward pass through. 

    <img class='center-image' src='/assets/img/ml/cnn/inception_v2.png' />
  </li>
</ul>

<p>
  Now you might be wondering how people came up with such models. As of
  writing this the experts in the field are largely also unsure as to why deep neural
  networks perform so well. As a result the design process for these new
  networks is typically adding more layers and trying new things with the
  network until something works. This process has been referred to as <a
  href='https://www.reddit.com/r/MachineLearning/comments/6hso7g/d_how_do_people_come_up_with_all_these_crazy_deep/'>Grad
  Student Descent</a> (a joke on gradient descent that mocks grad students
  iteratively guessing network architectures without reason to desperately get
  a paper published).
</p>

<h4>Applications in Society</h4>

<p>
  Now that we know what the sophisticated models are, we can get to
  where the real fun begins. Convolutional neural networks aren’t just a cool
  toy for CS nerds, or even an academic proof of concept: turns out, they can
  used to help solve problems beyond just the realm of computer science — and
  in many instances, they already have. When organizations have lots of
  spatially-structured data that they want to better understand, machine
  learning is there to lead the way.
</p>

<p>
  Below are some examples of how convolutional neural networks have been, and
  currently are being, applied in the real world. Our hope is that through
  continuing to use these cutting-edge technologies in positive ways, people will
  eventually come to see these technologies not as something to fear, but rather
  as a potential force for good. After going through this lesson, perhaps you,
  too, could think of a new way to use this exciting technology to help better
  the world around us.
</p>

<p>
  How are CNNs being used for social good? (Just to name a few examples)
  <ul>
    <li>
      Medicine: analyzing 3D lung scans to spot lung cancer <a
      href='https://www.kaggle.com/c/data-science-bowl-2017'>(Data Science Bowl 2017)</a>
    </li>
    <li>
      Environmentalism: detecting deforestation from satellite imagery 
      <a href='https://www.kaggle.com/c/planet-understanding-the-amazon-from-space'>(Kaggle Competition)</a>
    </li> 
    <li>
      Transportation: creating safer self-driving cars (NVIDIA: 
      <a href='https://arxiv.org/abs/1604.07316'>Paper</a>, 
      <a href='https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/'>Blog</a>)
    </li>
    <li>
      Wildlife Protection: training drones to patrol for poachers and animals from the sky 
      <a href='http://teamcore.usc.edu/projects/uavs/index.html'>(USC
      Teamcore)</a>
    </li>
  </ul>
</p>

<h4>Further Resources</h4>

<p>
  For further resources please see the following links to youtube videos:
</p>

<ul>
  <li>
    <a href='https://www.youtube.com/watch?v=FmpDIaiMIeA'>How Convolutional Neural Networks Work</a>
  </li>

  <li>
    <a href='https://www.youtube.com/watch?v=py5byOOHZM8&t=768s'>Neural Network
    that Changes Everything</a>
  </li>

  <li>
    <a href='https://www.youtube.com/watch?v=BFdMrDOx_CM&t=307s'>Inside a
    Neural Network</a>
  </li>
</ul>

<h2>Sources</h2>

<ul>
  <li>
    <a href='https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618'>
      Deep Learning (Adaptive Computation and Machine Learning Series) by
      Goodfellow, Bengio, Courville
    </a> 
  </li>
  <li>
    <a href='http://cs231n.stanford.edu/'>Stanford CS231n: Convolutional Neural
    Networks for Visual Recognition, Course Notes</a>
  </li>
</ul>
