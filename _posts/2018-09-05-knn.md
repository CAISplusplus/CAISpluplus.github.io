---
layout: post
published: true
title: "k-Nearest Neighbors"
mathjax: true
featured: true
comments: true
---

<h2><a name="introduction"></a>k-NN: k-Nearest Neighbors</h2>

k-Nearest Neighbors (k-NN) is one of the simplest machine learning algorithms. k-NN is commonly used for regression and classification problems, which are both types of supervised learning. In regression, the output is continuous (e.g. numerical values, such as house price), while in classification the output is discrete (e.g. classifying cats vs. dogs).

To illustrate how the k-NN algorithm works, let's consider a simple example of classifying points in the 2D plane into either a red or blue class. As shown in the figure below, our task is to classify the green point $$(4, 5)$$.

<img src="/images/knn1.png" style="width: 50%;"/>

In k-NN, $$k$$ is a hyperparameter: a parameter of the model that is chosen ahead of time and that can be tweaked during successive trials to improve performance. The crux of the k-NN algorithm is to examine the $$k$$ closest training examples to our test element. We measure closeness using feature similarity: how similar two elements are on the basis of their features.

In this example, our features are just the coordinates of our points. Recall that Euclidean distance between two points $$p = (p_{1}, p_{2},...,p_{n})$$ and $$q = (q_{1}, q_{2},...,q_{n})$$ is given by:

$$d(p,q) = d(q,p) = \sqrt{(p_{1} - q_{1})^2 + (p_{2} - q_{2})^2 + ... + (p_{n} - q_{n})^2}$$

Therefore, we can use Euclidean distance to find the $$k$$ closest points to the green test point. We will use $$k=3$$ in this example.

<img src="/images/knn2.png" style="width: 50%;"/>

For classification, the k-NN algorithm outputs the class most common among its $$k$$ nearest neighbors. Thus, because 2 of the 3 neighboring points we examined are red, we can classify our test point as red as well. As an extension, for regression problems, k-NN outputs the average of the values among its $$k$$ nearest neighbors.

<img src="/images/knn3.png" style="width: 50%;"/>

Choosing an appropriate value for $$k$$ is critical for the algorithm's performance and often depends on the number of training examples. A small value of k may cause the algorithm to be sensitive to noise in the dataset, while a large value may be computationally expensive.

To improve performance in cases where there is a skewed proportion of training examples per class, the distance of each neighbor is often weighted into the decision process. In addition, if features are not continuous variables, an overlap metric such as the [**Hamming Distance**](https://en.wikipedia.org/wiki/Hamming_distance) can be used. Despite its simplicity, k-NN often works surprisingly well for many real-life examples.

<h2>Sources</h2>
<ul>
  <li><a href='https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#Statistical_setting'>wikipedia.org/K-nearest_neighbors</a></li>
  <li><a href='https://medium.com/@adi.bronshtein/a-quick-introduction-to-k-nearest-neighbors-algorithm-62214cea29c7'>medium.com/a-quick-introduction-to-k-nearest-neighbors-algorithm</a></li>
  <li><a href='https://en.wikipedia.org/wiki/Hamming_distance'>wikipedia.org/Hamming_distance</a></li>
  <li><a href='https://en.wikipedia.org/wiki/Euclidean_distance'>wikipedia.org/Euclidean_distance</a></li>
  <li><a href='https://www.desmos.com/'>desmos.com</a></li>
</ul>
